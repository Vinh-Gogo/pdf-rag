#!/usr/bin/env python3
"""
retrieval_benchmark.py

Há»‡ thá»‘ng benchmark Ä‘Ã¡nh giÃ¡ retrieval cho cÃ¡c pages tÆ°Æ¡ng Ä‘á»“ng nháº¥t.
Bao gá»“m:
- Táº¡o ngÃ¢n hÃ ng cÃ¢u há»i ngáº«u nhiÃªn tá»« 4080 Ä‘oáº¡n vÄƒn báº£n
- HÃ m hybrid_search káº¿t há»£p similarity + BM25 ranking
- PhÆ°Æ¡ng thá»©c benchmark Ä‘Ã¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c cá»§a pipeline retrieval
"""

import os
import sys
import json
import random
import time
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict
import numpy as np

# Add project root to path
script_dir = Path(__file__).resolve().parent
project_root = script_dir.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document

# Load environment variables
load_dotenv()

# Import cÃ¡c hÃ m cáº§n thiáº¿t
from helpers.init_qdrant import qdrant_client, create_qdrant_vectorstore
from helpers.vectorstore_from_sequences import retrieve_similar_sequences

@dataclass
class QuestionItem:
    """Cáº¥u trÃºc cho má»™t cÃ¢u há»i trong question bank"""
    question_id: str
    question: str
    ground_truth_sequence_id: str
    ground_truth_content: str
    file_name: str
    category: str  # Loáº¡i cÃ¢u há»i: 'direct', 'paraphrase', 'summary', 'related'

@dataclass
class RetrievalResult:
    """Káº¿t quáº£ retrieval cho má»™t cÃ¢u há»i"""
    question_id: str
    retrieved_sequences: List[Dict[str, Any]]
    is_correct: bool
    rank: Optional[int]  # Vá»‹ trÃ­ cá»§a ground truth (1-based, None náº¿u khÃ´ng tÃ¬m tháº¥y)
    similarity_score: float
    retrieval_time: float

@dataclass
class BenchmarkResults:
    """Káº¿t quáº£ tá»•ng há»£p benchmark"""
    accuracy: Dict[str, float]  # hit@k rates
    mrr: float  # Mean Reciprocal Rank
    avg_retrieval_time: float
    category_performance: Dict[str, Dict[str, float]]
    detailed_results: List[RetrievalResult]

class QuestionBankGenerator:
    """Táº¡o ngÃ¢n hÃ ng cÃ¢u há»i ngáº«u nhiÃªn tá»« sequences"""

    def __init__(self, sequences_file: str):
        self.sequences_file = sequences_file
        self.sequences = self._load_sequences()

        # Templates cho cÃ¡c loáº¡i cÃ¢u há»i
        self.question_templates = {
            'direct': [
                "Ná»™i dung vá» '{keyword}' trong vÄƒn báº£n lÃ  gÃ¬?",
                "ThÃ´ng tin liÃªn quan Ä‘áº¿n '{keyword}' Ä‘Æ°á»£c Ä‘á» cáº­p nhÆ° tháº¿ nÃ o?",
                "Äoáº¡n vÄƒn báº£n nÃ³i vá» '{keyword}' cÃ³ ná»™i dung gÃ¬?",
            ],
            'paraphrase': [
                "Báº¡n cÃ³ thá»ƒ giáº£i thÃ­ch vá» '{keyword}' theo cÃ¡ch khÃ¡c khÃ´ng?",
                "Ã nghÄ©a cá»§a '{keyword}' trong tÃ i liá»‡u Ä‘Æ°á»£c thá»ƒ hiá»‡n ra sao?",
                "Ná»™i dung liÃªn quan Ä‘áº¿n '{keyword}' Ä‘Æ°á»£c mÃ´ táº£ nhÆ° tháº¿ nÃ o?",
            ],
            'summary': [
                "TÃ³m táº¯t ná»™i dung vá» '{keyword}' trong vÄƒn báº£n.",
                "Nhá»¯ng Ä‘iá»ƒm chÃ­nh vá» '{keyword}' Ä‘Æ°á»£c Ä‘á» cáº­p lÃ  gÃ¬?",
                "TÃ³m lÆ°á»£c thÃ´ng tin vá» '{keyword}' tá»« tÃ i liá»‡u.",
            ],
            'related': [
                "Nhá»¯ng váº¥n Ä‘á» liÃªn quan Ä‘áº¿n '{keyword}' Ä‘Æ°á»£c Ä‘á» cáº­p khÃ´ng?",
                "NgoÃ i '{keyword}', cÃ²n cÃ³ thÃ´ng tin gÃ¬ liÃªn quan?",
                "CÃ¡c khÃ­a cáº¡nh khÃ¡c cá»§a '{keyword}' Ä‘Æ°á»£c nÃ³i Ä‘áº¿n nhÆ° tháº¿ nÃ o?",
            ]
        }

    def _load_sequences(self) -> List[Dict[str, Any]]:
        """Load sequences tá»« file JSON"""
        with open(self.sequences_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _extract_keywords(self, content: str) -> List[str]:
        """Extract keywords tá»« content"""
        # Simple keyword extraction - cÃ³ thá»ƒ cáº£i thiá»‡n vá»›i NLP
        words = content.split()
        keywords = []

        # Láº¥y cÃ¡c tá»«/cá»¥m tá»« quan trá»ng
        for i in range(len(words)):
            if len(words[i]) > 3:  # Tá»« dÃ i hÆ¡n 3 kÃ½ tá»±
                keywords.append(words[i])
            # Láº¥y bigram
            if i < len(words) - 1:
                bigram = f"{words[i]} {words[i+1]}"
                if len(bigram) > 6:
                    keywords.append(bigram)

        return list(set(keywords[:10]))  # Láº¥y tá»‘i Ä‘a 10 keywords

    def generate_question_bank(self, num_questions: int = 1000,
                             category_distribution: Optional[Dict[str, float]] = None,
                             min_words: int = 20) -> List[QuestionItem]:
        """
        Táº¡o question bank vá»›i phÃ¢n phá»‘i categories

        Args:
            num_questions: Sá»‘ lÆ°á»£ng cÃ¢u há»i cáº§n táº¡o
            category_distribution: PhÃ¢n phá»‘i cÃ¡c loáº¡i cÃ¢u há»i
            min_words: Sá»‘ tá»« tá»‘i thiá»ƒu trong sequence Ä‘á»ƒ Ä‘Æ°á»£c chá»n
        """
        if category_distribution is None:
            category_distribution = {
                'direct': 0.4,
                'paraphrase': 0.3,
                'summary': 0.2,
                'related': 0.1
            }

        # Lá»c sequences cÃ³ Ä‘á»§ sá»‘ tá»«
        filtered_sequences = [
            seq for seq in self.sequences
            if len(seq['content'].split()) >= min_words
        ]

        print(f"ğŸ“Š Lá»c sequences: {len(self.sequences)} â†’ {len(filtered_sequences)} (min {min_words} words)")

        if len(filtered_sequences) == 0:
            print("âŒ KhÃ´ng cÃ³ sequence nÃ o Ä‘á»§ Ä‘iá»u kiá»‡n!")
            return []

        # TÃ­nh sá»‘ lÆ°á»£ng cÃ¢u há»i cho má»—i category
        category_counts = {}
        for cat, ratio in category_distribution.items():
            category_counts[cat] = int(num_questions * ratio)

        # Äiá»u chá»‰nh Ä‘á»ƒ tá»•ng báº±ng num_questions
        total = sum(category_counts.values())
        if total < num_questions:
            category_counts['direct'] += num_questions - total

        question_bank = []

        for category, count in category_counts.items():
            # Chá»n ngáº«u nhiÃªn sequences cho category nÃ y tá»« filtered list
            available_sequences = min(count, len(filtered_sequences))
            selected_sequences = random.sample(filtered_sequences, available_sequences)

            for seq in selected_sequences:
                keywords = self._extract_keywords(seq['content'])
                if not keywords:
                    continue

                keyword = random.choice(keywords)
                template = random.choice(self.question_templates[category])

                question = template.format(keyword=keyword)

                question_item = QuestionItem(
                    question_id=f"{category}_{seq['sequence_id']}_{random.randint(1000, 9999)}",
                    question=question,
                    ground_truth_sequence_id=seq['sequence_id'],
                    ground_truth_content=seq['content'],
                    file_name=seq['file_name'],
                    category=category
                )

                question_bank.append(question_item)

        random.shuffle(question_bank)
        return question_bank

class HybridRetriever:
    """Hybrid search combining BM25 + Vector similarity"""

    def __init__(self, vectorstore, sequences_data: List[Dict[str, Any]]):
        self.vectorstore = vectorstore
        self.sequences_data = sequences_data

        # Táº¡o BM25 retriever
        docs = [
            Document(
                page_content=seq['content'],
                metadata={
                    'sequence_id': seq['sequence_id'],
                    'file_name': seq['file_name'],
                    'sequence_number': seq['sequence_number']
                }
            )
            for seq in sequences_data
        ]

        self.bm25_retriever = BM25Retriever.from_documents(docs)

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Hybrid search vá»›i BM25 + Vector similarity

        Args:
            query: CÃ¢u há»i
            top_k: Sá»‘ lÆ°á»£ng káº¿t quáº£ tráº£ vá»

        Returns:
            List cÃ¡c káº¿t quáº£ vá»›i metadata Ä‘áº§y Ä‘á»§
        """
        # Thá»±c hiá»‡n BM25 search
        bm25_results = self.bm25_retriever.get_relevant_documents(query, k=top_k)

        # Thá»±c hiá»‡n vector search
        vector_results = self.vectorstore.similarity_search_with_score(query, k=top_k)

        # Káº¿t há»£p káº¿t quáº£ tá»« cáº£ hai
        combined_results = []

        # Xá»­ lÃ½ BM25 results
        for doc in bm25_results:
            sequence_id = doc.metadata.get('sequence_id')
            if sequence_id:
                # TÃ¬m sequence gá»‘c
                original_seq = None
                for seq in self.sequences_data:
                    if seq['sequence_id'] == sequence_id:
                        original_seq = seq
                        break

                if original_seq:
                    combined_results.append({
                        'doc': doc,
                        'source': 'bm25',
                        'sequence_id': sequence_id,
                        'original_seq': original_seq,
                        'score': 0.5  # Default score for BM25
                    })

        # Xá»­ lÃ½ vector results - sá»­ dá»¥ng Qdrant client Ä‘á»ƒ láº¥y full payload
        for doc, score in vector_results:
            # Láº¥y point ID tá»« metadata
            point_id = doc.metadata.get('_id')
            
            if point_id:
                try:
                    # Sá»­ dá»¥ng Qdrant client Ä‘á»ƒ láº¥y full payload
                    from store.init_qdrant import qdrant_client
                    point_data = qdrant_client.retrieve(
                        collection_name='esg_sequences',
                        ids=[point_id],
                        with_payload=True,
                        with_vectors=False
                    )
                    
                    if point_data and len(point_data) > 0:
                        payload = point_data[0].payload
                        if payload:
                            sequence_id = payload.get('sequence_id') or payload.get('id')
                        else:
                            sequence_id = None
                        
                        if sequence_id:
                            # TÃ¬m sequence gá»‘c tá»« sequences_data
                            original_seq = None
                            for seq in self.sequences_data:
                                if seq['sequence_id'] == sequence_id:
                                    original_seq = seq
                                    break
                            
                            if original_seq:
                                combined_results.append({
                                    'doc': doc,
                                    'source': 'vector',
                                    'sequence_id': sequence_id,
                                    'original_seq': original_seq,
                                    'score': float(score)
                                })
                            else:
                                print(f"WARNING: Could not find original sequence for sequence_id: {sequence_id}")
                        else:
                            print(f"WARNING: No sequence_id in payload: {payload}")
                    else:
                        print(f"WARNING: Could not retrieve point data for ID: {point_id}")
                        
                except Exception as e:
                    print(f"WARNING: Error retrieving payload for point {point_id}: {e}")
            else:
                print(f"WARNING: No _id found in vector result metadata: {doc.metadata}")

        # Loáº¡i bá» duplicates vÃ  láº¥y top káº¿t quáº£
        seen_ids = set()
        unique_results = []

        # Sáº¯p xáº¿p theo score (vector results cÃ³ score thá»±c, BM25 cÃ³ score máº·c Ä‘á»‹nh)
        combined_results.sort(key=lambda x: x.get('score', 0.5), reverse=True)

        for result in combined_results:
            seq_id = result['sequence_id']
            if seq_id not in seen_ids:
                seen_ids.add(seq_id)
                unique_results.append(result)
                if len(unique_results) >= top_k:
                    break

        # Format káº¿t quáº£ cuá»‘i cÃ¹ng
        formatted_results = []
        for result in unique_results:
            original_seq = result['original_seq']
            formatted_result = {
                'id': original_seq['sequence_id'],
                'content': original_seq['content'],
                'file_name': original_seq['file_name'],
                'sequence_number': original_seq['sequence_number'],
                'total_sequences_in_file': original_seq['total_sequences_in_file'],
                'similarity_score': result.get('score', 0.5),
                'source': result['source']
            }
            formatted_results.append(formatted_result)

        return formatted_results

class RetrievalBenchmark:
    """Benchmark Ä‘Ã¡nh giÃ¡ retrieval performance"""

    def __init__(self, retriever: HybridRetriever, question_bank: List[QuestionItem]):
        self.retriever = retriever
        self.question_bank = question_bank

    def evaluate_single_question(self, question_item: QuestionItem, top_k: int = 5) -> RetrievalResult:
        """ÄÃ¡nh giÃ¡ má»™t cÃ¢u há»i"""
        start_time = time.time()

        # Thá»±c hiá»‡n retrieval
        retrieved_sequences = self.retriever.search(question_item.question, top_k=top_k)

        retrieval_time = time.time() - start_time

        # Kiá»ƒm tra xem ground truth cÃ³ trong káº¿t quáº£ khÃ´ng
        rank = None
        is_correct = False
        similarity_score = 0.0

        for i, result in enumerate(retrieved_sequences, 1):
            if result['id'] == question_item.ground_truth_sequence_id:
                rank = i
                is_correct = True
                similarity_score = result['similarity_score']
                break

        return RetrievalResult(
            question_id=question_item.question_id,
            retrieved_sequences=retrieved_sequences,
            is_correct=is_correct,
            rank=rank,
            similarity_score=similarity_score,
            retrieval_time=retrieval_time
        )

    def run_benchmark(self, top_k_values: List[int] = [1, 3, 5, 10]) -> BenchmarkResults:
        """Cháº¡y benchmark vá»›i táº¥t cáº£ cÃ¢u há»i"""
        print(f"ğŸš€ Báº¯t Ä‘áº§u benchmark vá»›i {len(self.question_bank)} cÃ¢u há»i...")

        detailed_results = []
        retrieval_times = []

        for i, question_item in enumerate(self.question_bank, 1):
            if i % 50 == 0:
                print(f"ğŸ“Š ÄÃ£ xá»­ lÃ½ {i}/{len(self.question_bank)} cÃ¢u há»i...")

            result = self.evaluate_single_question(question_item, max(top_k_values))
            detailed_results.append(result)
            retrieval_times.append(result.retrieval_time)

        # TÃ­nh toÃ¡n metrics
        accuracy = {}
        for k in top_k_values:
            correct_count = sum(1 for r in detailed_results if r.rank and r.rank <= k)
            accuracy[f'hit@{k}'] = correct_count / len(detailed_results)

        # Mean Reciprocal Rank
        mrr = np.mean([
            1.0 / result.rank if result.rank else 0.0
            for result in detailed_results
        ])

        # Category performance
        category_performance = defaultdict(lambda: defaultdict(float))
        category_counts = defaultdict(int)

        for result in detailed_results:
            question_item = next(q for q in self.question_bank if q.question_id == result.question_id)
            category = question_item.category
            category_counts[category] += 1

            for k in top_k_values:
                if result.rank and result.rank <= k:
                    category_performance[category][f'hit@{k}'] += 1

        # Normalize category performance
        for category in category_performance:
            for k in top_k_values:
                if category_counts[category] > 0:
                    category_performance[category][f'hit@{k}'] /= category_counts[category]

        return BenchmarkResults(
            accuracy=dict(accuracy),
            mrr=float(mrr),
            avg_retrieval_time=float(np.mean(retrieval_times)),
            category_performance=dict(category_performance),
            detailed_results=detailed_results
        )

def print_benchmark_results(results: BenchmarkResults):
    """In káº¿t quáº£ benchmark"""
    print("\n" + "="*80)
    print("ğŸ“Š Káº¾T QUáº¢ BENCHMARK RETRIEVAL")
    print("="*80)

    print("\nğŸ¯ ACCURACY METRICS:")
    for metric, value in results.accuracy.items():
        print(f"   {metric}: {value:.1%}")

    print(f"\nğŸ“Š MRR: {results.mrr:.4f}")
    print(f"â±ï¸  Avg Retrieval Time: {results.avg_retrieval_time:.2f}s")

    print("\nğŸ“ˆ CATEGORY PERFORMANCE:")
    for category, metrics in results.category_performance.items():
        print(f"\nğŸ·ï¸  {category.upper()}:")
        for metric, value in metrics.items():
            print(f"   {metric}: {value:.1%}")

    print("\nğŸ“‹ THá»NG KÃŠ CHI TIáº¾T:")
    total_questions = len(results.detailed_results)
    correct_questions = sum(1 for r in results.detailed_results if r.is_correct)
    avg_similarity = np.mean([r.similarity_score for r in results.detailed_results if r.is_correct])

    print(f"   Tá»•ng sá»‘ cÃ¢u há»i: {total_questions}")
    print(f"   Sá»‘ cÃ¢u tráº£ lá»i Ä‘Ãºng: {correct_questions}")
    print(f"   Tá»· lá»‡ chÃ­nh xÃ¡c: {correct_questions/total_questions:.1%}")
    print(f"   Äiá»ƒm tÆ°Æ¡ng Ä‘á»“ng trung bÃ¬nh: {avg_similarity:.4f}")

def save_benchmark_results(results: BenchmarkResults, output_file: str):
    """LÆ°u káº¿t quáº£ benchmark vÃ o file JSON"""
    # Convert dataclasses to dicts
    results_dict = {
        'accuracy': results.accuracy,
        'mrr': results.mrr,
        'avg_retrieval_time': results.avg_retrieval_time,
        'category_performance': results.category_performance,
        'detailed_results': [
            {
                'question_id': r.question_id,
                'is_correct': r.is_correct,
                'rank': r.rank,
                'similarity_score': r.similarity_score,
                'retrieval_time': r.retrieval_time,
                'num_retrieved': len(r.retrieved_sequences)
            }
            for r in results.detailed_results
        ]
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results_dict, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ ÄÃ£ lÆ°u káº¿t quáº£ benchmark vÃ o {output_file}")

def main():
    """Main function Ä‘á»ƒ cháº¡y benchmark"""
    print("ğŸš€ KHá»I Äá»˜NG Há»† THá»NG BENCHMARK RETRIEVAL")
    print("="*60)

    # ÄÆ°á»ng dáº«n files
    script_dir = Path(__file__).resolve().parent
    project_root = script_dir.parent.parent  # Go up two levels to get to project root
    sequences_file = project_root / "src" / "data" / "push" / "merged_sequences.json"

    # Kiá»ƒm tra file sequences
    if not sequences_file.exists():
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file sequences: {sequences_file}")
        return

    print(f"ğŸ“‚ File sequences: {sequences_file}")

    # 1. Táº¡o question bank
    print("\nğŸ“ Táº O QUESTION BANK...")
    question_generator = QuestionBankGenerator(str(sequences_file))
    question_bank = question_generator.generate_question_bank(num_questions=500, min_words=20)

    print(f"âœ… ÄÃ£ táº¡o {len(question_bank)} cÃ¢u há»i")
    print("ğŸ“Š PhÃ¢n phá»‘i categories:")
    category_counts = defaultdict(int)
    for q in question_bank:
        category_counts[q.category] += 1

    for cat, count in category_counts.items():
        print(f"   {cat}: {count} cÃ¢u há»i")

    # 2. Khá»Ÿi táº¡o retriever
    print("\nğŸ”§ KHá»I Táº O HYBRID RETRIEVER...")
    try:
        vectorstore = create_qdrant_vectorstore('esg_sequences')
        print("âœ… ÄÃ£ káº¿t ná»‘i Ä‘áº¿n vector store")
    except Exception as e:
        print(f"âŒ Lá»—i káº¿t ná»‘i vector store: {e}")
        return

    # Load sequences data
    with open(sequences_file, 'r', encoding='utf-8') as f:
        sequences_data = json.load(f)

    retriever = HybridRetriever(vectorstore, sequences_data)
    print("âœ… ÄÃ£ khá»Ÿi táº¡o hybrid retriever")

    # 3. Cháº¡y benchmark
    print("\nğŸƒ CHáº Y BENCHMARK...")
    benchmark = RetrievalBenchmark(retriever, question_bank)
    results = benchmark.run_benchmark(top_k_values=[1, 3, 5, 10])

    # 4. Hiá»ƒn thá»‹ káº¿t quáº£
    print_benchmark_results(results)

    # 5. LÆ°u káº¿t quáº£
    output_file = project_root / "src" / "eval" / "benchmark_results.json"
    save_benchmark_results(results, str(output_file))

    print("\nâœ… HOÃ€N THÃ€NH BENCHMARK!")
    print(f"ğŸ“Š Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u táº¡i: {output_file}")

if __name__ == "__main__":
    main()