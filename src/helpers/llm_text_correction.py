from transformers import AutoModelForCausalLM, AutoTokenizer
import torch, os
from pathlib import Path
import re
# from utils.helper import Helpers
# from multiprocessing import Pool

r"""
```c
FILE -> src\data\results\grammar
NEXT:
grammar\page_1.txt -> src\data\contents\page_1_clear.txt
```
"""

# ‚öôÔ∏è GPU config
torch.set_float32_matmul_precision("high")

torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True
torch.backends.cudnn.benchmark = True

# üîß T·∫£i model/tokenizer m·ªôt l·∫ßn duy nh·∫•t
# model_name = "Qwen/Qwen3-4B-Instruct-2507"
model_name = "Qwen/Qwen2.5-1.5B-Instruct"
# model_name = "Qwen/Qwen2.5-0.5B-Instruct"
# model_name = "Qwen/Qwen3-0.6B"
# model_name = "Gensyn/Qwen2.5-0.5B-Instruct"

# üöÄ C√ÅC T√ôY CH·ªåN C·∫¢I THI·ªÜN HI·ªÜU SU·∫§T:
# 1. Th·ª≠ m√¥ h√¨nh l·ªõn h∆°n: "Qwen/Qwen2.5-3B-Instruct", "Qwen/Qwen2.5-7B-Instruct"
# 2. Th·ª≠ m√¥ h√¨nh chuy√™n bi·ªát ti·∫øng Vi·ªát: "vinai/PhoGPT-7B5-Instruct" (n·∫øu c√≥)

# üìÇ ƒê·ªçc file .txt ƒë·∫ßu v√†o
def read_all_txt_list(folder: str) -> dict[int, str]:
    files = sorted([f for f in os.listdir(folder) if f.endswith("clear.txt") and "_ocr" not in f], 
                   key=lambda x: int(x.split('_')[1].split('.')[0]))
    return {int(f.split('_')[1].split('.')[0]): open(os.path.join(folder, f), encoding="utf-8").read() 
            for f in files}

import os
import re
from typing import List

def extract_unique_words(raw_folder: str) -> List[str]:
    """
    ƒê·ªçc t·∫•t c·∫£ file .txt trong folder v√† tr√≠ch xu·∫•t t·∫≠p h·ª£p c√°c t·ª´ duy nh·∫•t.
    
    Args:
        raw_folder (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn folder ch·ª©a c√°c file .txt
        
    Returns:
        Set[str]: T·∫≠p h·ª£p c√°c t·ª´ duy nh·∫•t (ch·ªâ ch·ª©a ch·ªØ c√°i ti·∫øng Vi·ªát)
    """
    char_counter = set()
    
    for filename in sorted(os.listdir(raw_folder)):
        if filename.endswith('.txt'):
            file_path = os.path.join(raw_folder, filename)
            
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            words = content.split()
            
            # Lo·∫°i b·ªè k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ c√°i
            words = [re.sub(r'[^a-zA-Z√Ä-·ªπ√†-·ªπ0-9]', '', word) for word in words]
            
            # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát v√† s·ªë, ch·ªâ gi·ªØ t·ª´ ch·ª©a ch·ªØ c√°i
            words = [word for word in words if word.isalpha() and not word.isdigit()]
            
            # Th√™m v√†o t·∫≠p h·ª£p
            char_counter = char_counter.union(set(words))
    
    return list(char_counter)

unique_words = extract_unique_words(r"src\data\raw")

def tokenize_words(text: str) -> list:
    """T√°ch vƒÉn b·∫£n th√†nh danh s√°ch c√°c t·ª´ (lo·∫°i b·ªè d·∫•u c√¢u v√† k√Ω t·ª± ƒë·∫∑c bi·ªát)"""
    text_lower = text.lower()
    words = re.findall(r'\b\w+\b', text_lower)
    return words

def create_word_dictionary(text: str) -> set:
    """
    T·∫°o t·ª´ ƒëi·ªÉn (set) c√°c t·ª´ duy nh·∫•t t·ª´ vƒÉn b·∫£n g·ªëc
    """
    words = tokenize_words(text)
    return set(words)

def detect_english_words(text: str) -> list:
    """
    Ph√°t hi·ªán c√°c t·ª´ ti·∫øng Anh nguy√™n b·∫£n r√µ r√†ng (lo·∫°i b·ªè danh t·ª´ qu·ªëc t·∫ø)
    
    Returns:
        list: Danh s√°ch c√°c t·ª´ ti·∫øng Anh t√¨m th·∫•y (ch·ªâ nh·ªØng t·ª´ nguy√™n b·∫£n ti·∫øng Anh)
    """
    # Danh s√°ch c√°c t·ª´ ti·∫øng Anh NGUY√äN B·∫¢N - c√°c t·ª´ ƒë∆°n gi·∫£n, ƒë·ªông t·ª´, gi·ªõi t·ª´, li√™n t·ª´
    # Lo·∫°i b·ªè c√°c danh t·ª´ qu·ªëc t·∫ø th∆∞·ªùng d√πng trong ti·∫øng Vi·ªát (email, website, etc.)
    pure_english_words = {
        # Articles, pronouns
        'the', 'i', 'you', 'he', 'she', 'it', 'we', 'they',
        'me', 'him', 'her', 'us', 'them'
        # Common verbs
        'is', 'are', 'was', 'were', 'be', 'been', 'being', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
        'am',
        'have', 'has', 'does', 'did',
        'will', 'would', 'should', 'could', 'might', 'must', 'can',
        'get', 'got', 'make', 'made', 'take', 'took', 'come', 'came', 'go', 'went',
        'said', 'see', 'saw', 'know', 'knew', 'think', 'thought',
        # Common conjunctions, prepositions
        'and', 'or', 'but', 'yet',
        'on', 'at', 'for', 'by', 'from', 'with', 'as',
        'about', 'after', 'before', 'between', 'during', 'through',
        'into', 'over', 'under', 'above', 'below', 'off', 'out', 'up', 'down',
        # Common adverbs, quantifiers
        'not', 'yes', 'all', 'each', 'every', 'both', 'few', 'more', 'most',
        'other', 'some', 'such', 'only', 'same', 'just', 'very', 'how', 'why',
        'where', 'when', 'what', 'which', 'who', 'whom', 'whose', 'get', 'got', 'getting', 'make', 'made', 'making', 'take', 'took', 'taking',
        'come', 'came', 'coming', 'go', 'went', 'going', 'said', 'saying',
        'see', 'saw', 'seeing', 'know', 'knew', 'knowing', 'think', 'thought', 'thinking',
        'give', 'gave', 'giving', 'find', 'found', 'finding', 'tell', 'told', 'telling',
        'ask', 'asked', 'asking', 'work', 'worked', 'working', 'call', 'called', 'calling',
        'try', 'tried', 'trying', 'use', 'used', 'using', 'feel', 'felt', 'feeling',
        'become', 'became', 'becoming', 'leave', 'left', 'leaving', 'put', 'putting',
        'mean', 'meant', 'meaning', 'keep', 'kept', 'keeping', 'help', 'helped', 'helping',
        'talk', 'talked', 'talking', 'turn', 'turned', 'turning', 'start', 'started', 'starting',
        'show', 'showed', 'showing', 'hear', 'heard', 'hearing', 'let', 'letting', 'hold', 'held', 'holding',
        'bring', 'brought', 'bringing', 'begin', 'began', 'beginning', 'seem', 'seemed', 'seeming',
        'write', 'wrote', 'writing', 'written', 'provide', 'provided', 'providing',
        'play', 'played', 'playing', 'run', 'ran', 'running', 'move', 'moved', 'moving',
        'like', 'liked', 'liking', 'live', 'lived', 'living', 'believe', 'believed', 'believing',
        'want', 'wanted', 'wanting', 'look', 'looked', 'looking', 'appear', 'appeared', 'appearing',
        'watch', 'watched', 'watching', 'follow', 'followed', 'following', 'stop', 'stopped', 'stopping',
        'create', 'created', 'creating', 'speak', 'spoke', 'speaking', 'read', 'reading',
        'allow', 'allowed', 'allowing', 'add', 'added', 'adding', 'spend', 'spent', 'spending',
        'grow', 'grew', 'growing', 'grown', 'draw', 'drew', 'drawing', 'drawn', 'break', 'broke', 'breaking', 'broken',
        'happen', 'happened', 'happening', 'choose', 'chose', 'choosing', 'chosen', 'deal', 'dealt', 'dealing',
        'serve', 'served', 'serving', 'eat', 'ate', 'eating', 'eaten', 'cover', 'covered', 'covering',
        'catch', 'caught', 'catching', 'draw', 'draw', 'drive', 'drove', 'driving', 'driven',
        'die', 'died', 'dying', 'face', 'faced', 'facing', 'fail', 'failed', 'failing',
        'gain', 'gained', 'gaining', 'hang', 'hung', 'hanging', 'hit', 'hitting',
        'hold', 'hole', 'hunt', 'hunted', 'hunting', 'include', 'included', 'including',
        'increase', 'increased', 'increasing', 'involve', 'involved', 'involving', 'join', 'joined', 'joining',
        'jump', 'jumped', 'jumping', 'kill', 'killed', 'killing', 'laid', 'laying',
        'lead', 'led', 'leading', 'learn', 'learned', 'learning', 'learnt', 'leave', 'left',
        'light', 'lit', 'lighting', 'lighted', 'listen', 'listened', 'listening', 'lose', 'lost', 'losing',
        'love', 'loved', 'loving', 'measure', 'measured', 'measuring', 'meet', 'met', 'meeting',
        'mind', 'minded', 'minding', 'miss', 'missed', 'missing', 'obtain', 'obtained', 'obtaining',
        'occur', 'occurred', 'occurring', 'offer', 'offered', 'offering', 'open', 'opened', 'opening',
        'order', 'ordered', 'ordering', 'own', 'owned', 'owning', 'paint', 'painted', 'painting',
        'pass', 'passed', 'passing', 'pay', 'paid', 'paying', 'perform', 'performed', 'performing',
        'perhaps', 'pick', 'picked', 'picking', 'point', 'pointed', 'pointing', 'prepare', 'prepared', 'preparing',
        'present', 'presented', 'presenting', 'prevent', 'prevented', 'preventing', 'print', 'printed', 'printing',
        'promise', 'promised', 'promising', 'prove', 'proved', 'proving', 'proven', 'pull', 'pulled', 'pulling',
        'push', 'pushed', 'pushing', 'raise', 'raised', 'raising', 'reach', 'reached', 'reaching',
        'realize', 'realized', 'realizing', 'receive', 'received', 'receiving', 'record', 'recorded', 'recording',
        'reduce', 'reduced', 'reducing', 'refuse', 'refused', 'refusing', 'regard', 'regarded', 'regarding',
        'remember', 'remembered', 'remembering', 'remove', 'removed', 'removing', 'repeat', 'repeated', 'repeating',
        'replace', 'replaced', 'replacing', 'report', 'reported', 'reporting', 'require', 'required', 'requiring',
        'result', 'resulted', 'resulting', 'return', 'returned', 'returning', 'reveal', 'revealed', 'revealing',
        'review', 'reviewed', 'reviewing', 'ride', 'rode', 'riding', 'ridden', 'ring', 'rang', 'ringing', 'rung',
        'rise', 'rose', 'rising', 'risen', 'risk', 'risked', 'risking', 'roll', 'rolled', 'rolling',
        'rub', 'rubbed', 'rubbing', 'rule', 'ruled', 'ruling', 'rush', 'rushed', 'rushing',
        'sail', 'sailed', 'sailing', 'satisfy', 'satisfied', 'satisfying', 'save', 'saved', 'saving',
        'search', 'searched', 'searching', 'seat', 'seated', 'seating', 'secure', 'secured', 'securing',
        'seek', 'sought', 'seeking', 'seem', 'seemed', 'seeming', 'seize', 'seized', 'seizing',
        'sell', 'sold', 'selling', 'send', 'sent', 'sending', 'sense', 'sensed', 'sensing',
        'separate', 'separated', 'separating', 'set', 'setting', 'settle', 'settled', 'settling',
        'shake', 'shook', 'shaking', 'shaken', 'share', 'shared', 'sharing', 'shift', 'shifted', 'shifting',
        'shine', 'shone', 'shining', 'shined', 'ship', 'shipped', 'shipping', 'shoot', 'shot', 'shooting',
        'shop', 'shopped', 'shopping', 'shut', 'shutting', 'sight', 'sighted', 'sighting',
        'sign', 'signed', 'signing', 'signal', 'signaled', 'signaling', 'sing', 'singing',
        'sink', 'sank', 'sinking', 'sunken', 'sit', 'sat', 'sitting', 'size', 'sized', 'sizing',
        'sketch', 'sketched', 'sketching', 'sleep', 'slept', 'sleeping', 'slide', 'slid', 'sliding',
        'smile', 'smiled', 'smiling', 'smoke', 'smoked', 'smoking', 'smooth', 'smoothed', 'smoothing',
        'snow', 'snowed', 'snowing', 'solve', 'solved', 'solving', 'sort', 'sorted', 'sorting',
        'sound', 'sounded', 'sounding', 'speak', 'spoke', 'speaking', 'spoken', 'speed', 'sped', 'speeding',
        'spend', 'spent', 'spending', 'spell', 'spelled', 'spelling', 'spelt', 'split', 'splitting',
        'spread', 'spreading', 'spring', 'sprung', 'springing', 'stand', 'stood', 'standing',
        'stare', 'stared', 'staring', 'start', 'started', 'starting', 'state', 'stated', 'stating',
        'stay', 'stayed', 'staying', 'steal', 'stole', 'stealing', 'stolen', 'step', 'stepped', 'stepping',
        'stick', 'stuck', 'sticking', 'still', 'stilled', 'stilling', 'sting', 'stung', 'stinging',
        'stink', 'stank', 'stinking', 'stunk', 'stitch', 'stitched', 'stitching', 'stop', 'stopped', 'stopping',
        'store', 'stored', 'storing', 'storm', 'stormed', 'storming', 'strain', 'strained', 'straining',
        'stream', 'streamed', 'streaming', 'street', 'stretch', 'stretched', 'stretching', 'strike', 'struck', 'striking', 'struck',
        'string', 'strung', 'stringing', 'strip', 'stripped', 'stripping', 'stroke', 'stroked', 'stroking',
        'struggle', 'struggled', 'struggling', 'study', 'studied', 'studying', 'stuff', 'stuffed', 'stuffing',
        'stumble', 'stumbled', 'stumbling', 'submit', 'submitted', 'submitting', 'succeed', 'succeeded', 'succeeding',
        'suck', 'sucked', 'sucking', 'suffer', 'suffered', 'suffering', 'suggest', 'suggested', 'suggesting',
        'suit', 'suited', 'suiting', 'sum', 'summed', 'summing', 'supply', 'supplied', 'supplying',
        'support', 'supported', 'supporting', 'suppose', 'supposed', 'supposing', 'suppress', 'suppressed', 'suppressing',
        'sure', 'surely', 'surface', 'surfaced', 'surfacing', 'surge', 'surged', 'surging',
        'surprise', 'surprised', 'surprising', 'surround', 'surrounded', 'surrounding', 'survey', 'surveyed', 'surveying',
        'survive', 'survived', 'surviving', 'suspect', 'suspected', 'suspecting', 'suspend', 'suspended', 'suspending',
        'sustain', 'sustained', 'sustaining', 'swallow', 'swallowed', 'swallowing', 'swear', 'swore', 'swearing', 'sworn',
        'sweat', 'sweated', 'sweating', 'sweep', 'swept', 'sweeping', 'swell', 'swelled', 'swelling', 'swollen',
        'swim', 'swam', 'swimming', 'swum', 'swing', 'swung', 'swinging', 'switch', 'switched', 'switching',
        'swoop', 'swooped', 'swooping', 'symbol', 'sympathize', 'sympathized', 'sympathizing', 'symptom',
        'sync', 'synced', 'syncing', 'system', 'systematize', 'systematized', 'systematizing', 'table', 'tabled', 'tabling',
        'tackle', 'tackled', 'tackling', 'tag', 'tagged', 'tagging', 'tail', 'tailed', 'tailing',
        'take', 'took', 'taking', 'taken', 'tale', 'talk', 'talked', 'talking', 'tally', 'tallied', 'tallying',
        'tame', 'tamed', 'taming', 'tan', 'tanned', 'tanning', 'tangle', 'tangled', 'tangling',
        'tap', 'tapped', 'tapping', 'tape', 'taped', 'taping', 'target', 'targeted', 'targeting',
        'task', 'tasked', 'tasking', 'taste', 'tasted', 'tasting', 'tattoo', 'tattooed', 'tattooing',
        'teach', 'taught', 'teaching', 'tease', 'teased', 'teasing', 'telephone', 'telephoned', 'telephoning',
        'tell', 'told', 'telling', 'temper', 'tempered', 'tempering', 'tempt', 'tempted', 'tempting',
        'tend', 'tended', 'tending', 'tender', 'tendered', 'tendering', 'tense', 'tensed', 'tensing',
        'term', 'termed', 'terming', 'terrify', 'terrified', 'terrifying', 'test', 'tested', 'testing',
        'text', 'texted', 'texting', 'thank', 'thanked', 'thanking', 'thaw', 'thawed', 'thawing',
        'theater', 'theft', 'theme', 'theory', 'therapy', 'there', 'therefore', 'thermal',
        'think', 'thought', 'thinking', 'thin', 'thinned', 'thinning', 'thirst', 'thirsted', 'thirsting',
        'thorn', 'thorough', 'those', 'thread', 'threaded', 'threading', 'threat', 'threatened', 'threatening',
        'three', 'thresh', 'threshed', 'threshing', 'threshold', 'threw', 'thrice', 'thrift',
        'thrill', 'thrilled', 'thrilling', 'thrive', 'thrived', 'thriving', 'throve', 'throb', 'throbbed', 'throbbing',
        'throne', 'throng', 'thronged', 'thronging', 'throttle', 'throttled', 'throttling', 'through', 'throw', 'threw', 'throwing', 'thrown',
        'thrust', 'thrusting', 'thumb', 'thumbed', 'thumbing', 'thump', 'thumped', 'thumping',
        'thunder', 'thundered', 'thundering', 'thus', 'thwart', 'thwarted', 'thwarting', 'ticket', 'ticketed', 'ticketing',
        'tickle', 'tickled', 'tickling', 'tide', 'tided', 'tiding', 'tidy', 'tidied', 'tidying',
        'tie', 'tied', 'tying', 'tier', 'tiered', 'tiering', 'tiger', 'tight', 'tighten', 'tightened', 'tightening',
        'tights', 'tile', 'tiled', 'tiling', 'till', 'tilled', 'tilling', 'tilt', 'tilted', 'tilting',
        'timber', 'time', 'timed', 'timing', 'timid', 'tined', 'tinfoil', 'tinge', 'tinged', 'tingeing', 'tingle', 'tingled', 'tingling',
        'tinker', 'tinkered', 'tinkering', 'tint', 'tinted', 'tinting', 'tiny', 'tip', 'tipped', 'tipping',
        'tipsy', 'tire', 'tired', 'tiring', 'tissue', 'titan', 'titanic', 'tithe', 'tithed', 'tithing',
        'title', 'titled', 'titling', 'titter', 'tittered', 'tittering', 'toad', 'toast', 'toasted', 'toasting',
        'tobacco', 'today', 'toddle', 'toddled', 'toddling', 'toe', 'toed', 'toeing', 'toffee',
        'tofu', 'together', 'toggle', 'toggled', 'toggling', 'toil', 'toiled', 'toiling', 'token',
        'tolerate', 'tolerated', 'tolerating', 'toll', 'tolled', 'tolling', 'tomato', 'tomb', 'tombed', 'tombing',
        'tombstone', 'tomcat', 'tome', 'tomorrow', 'ton', 'tone', 'toned', 'toning', 'tongs',
        'tongue', 'tonic', 'tonight', 'tonnage', 'tonsil', 'too', 'took', 'tool', 'tooled', 'tooling',
        'toot', 'tooted', 'tooting', 'tooth', 'toothbrush', 'toothpaste', 'toothpick', 'toots', 'top', 'topped', 'topping',
        'topic', 'topical', 'topography', 'topple', 'toppled', 'toppling', 'topsy', 'torch', 'torched', 'torching',
        'tore', 'torment', 'tormented', 'tormenting', 'torn', 'tornado', 'torpedo', 'torpedoed', 'torpedoing',
        'torpedo', 'torpor', 'torque', 'torrent', 'torrid', 'torso', 'tort', 'tortoise', 'torture', 'tortured', 'torturing',
        'torus', 'toss', 'tossed', 'tossing', 'tot', 'total', 'totaled', 'totaling', 'totalitarian',
        'totality', 'totality', 'tote', 'toted', 'toting', 'totem', 'totter', 'tottered', 'tottering',
        'toucan', 'touch', 'touched', 'touching', 'touchy', 'tough', 'toughen', 'toughened', 'toughening',
        'toughness', 'tour', 'toured', 'touring', 'tourism', 'tourist', 'tournament', 'tousle', 'tousled', 'tousling',
        'tout', 'touted', 'touting', 'tow', 'towed', 'towing', 'towage', 'toward', 'towards',
        'towel', 'toweled', 'toweling', 'towelled', 'towelling', 'tower', 'towered', 'towering',
        'town', 'township', 'toxic', 'toxin', 'toy', 'toyed', 'toying', 'trace', 'traced', 'tracing',
        'tracer', 'trachea', 'track', 'tracked', 'tracking', 'tract', 'traction', 'tractor',
        'trade', 'traded', 'trading', 'trader', 'tradition', 'traditional', 'traffic', 'trafficked', 'trafficking',
        'trafficker', 'tragedy', 'tragic', 'tragically', 'trail', 'trailed', 'trailing', 'trailer',
        'train', 'trained', 'training', 'trainee', 'trainer', 'traipse', 'traipsed', 'traipsing',
        'trait', 'traitor', 'traitorous', 'trajectory', 'tram', 'tramcar', 'tramp', 'tramped', 'tramping',
        'trample', 'trampled', 'trampling', 'trampoline', 'trampolined', 'trampolining', 'trance', 'tranquil',
        'tranquilizer', 'tranquilize', 'tranquilized', 'tranquillizing', 'tranquillity', 'transact', 'transacted', 'transacting',
        'transaction', 'transcend', 'transcended', 'transcending', 'transcendent', 'transcendental', 'transcendence',
        'transcontinental', 'transcribe', 'transcribed', 'transcribing', 'transcript', 'transcription', 'transcriber',
        'transducer', 'transeunt', 'transept', 'transfer', 'transferred', 'transferring', 'transferable', 'transference',
        'transfiguration', 'transfigure', 'transfigured', 'transfiguring', 'transfix', 'transfixed', 'transfixing',
        'transform', 'transformed', 'transforming', 'transformation', 'transformer', 'transfusion', 'transgress',
        'transgressed', 'transgressing', 'transgression', 'transgressor', 'tranship', 'transshipped', 'transhipping',
        'transhumance', 'transience', 'transient', 'transistor', 'transit', 'transited', 'transiting',
        'transition', 'transitioned', 'transitioning', 'transitional', 'transitive', 'transitivity', 'transitory',
        'translatable', 'translate', 'translated', 'translating', 'translation', 'translator', 'transliterate',
        'transliterated', 'transliterating', 'transliteration', 'translucence', 'translucency', 'translucent',
        'transmigration', 'transmigrate', 'transmigrated', 'transmigrating', 'transmissible', 'transmission',
        'transmit', 'transmitted', 'transmitting', 'transmitter', 'transmittal', 'transmittable', 'transmogrification',
        'transmogrify', 'transmogrified', 'transmogrifying', 'transmontane', 'transmutation', 'transmute',
        'transmuted', 'transmuting', 'transnational', 'transoceanic', 'transom', 'transparency', 'transparent',
        'transpicuous', 'transpiration', 'transpire', 'transpired', 'transpiring', 'transplant', 'transplanted', 'transplanting',
        'transplantation', 'transplanter', 'transplant', 'transplanter', 'transpolar', 'transpontine', 'transport',
        'transported', 'transporting', 'transportable', 'transportation', 'transporter', 'transpontine', 'transpose',
        'transposed', 'transposing', 'transposition', 'transposal', 'transposable', 'transylvania',
        'transylvanian', 'truncyear', 'transferals', 'tansy', 'trans'
    }
    
    words = tokenize_words(text)
    english_words = [w for w in words if w in pure_english_words]
    return english_words

def check_vocabulary_match(original: str, corrected: str) -> dict:
    """
    Ki·ªÉm tra xem t·∫•t c·∫£ c√°c t·ª´ sau x·ª≠ l√Ω c√≥ n·∫±m trong t·ª´ ƒëi·ªÉn c·ªßa vƒÉn b·∫£n g·ªëc kh√¥ng
    Ch·ªâ t√≠nh ch·ªØ c√°i, kh√¥ng t√≠nh s·ªë
    KH√îNG PH√ÇN BI·ªÜT CH·ªÆ HOA CH·ªÆ TH∆Ø·ªúNG
    
    Returns:
        dict: {
            'original_vocab_size': int,
            'corrected_vocab_size': int,
            'new_words_count': int,
            'new_words': list,
            'all_words_in_dict': bool,
            'english_words': list
        }
    """
    # Chuy·ªÉn t·∫•t c·∫£ v·ªÅ lowercase ƒë·ªÉ so s√°nh
    original_dict = create_word_dictionary(original.lower())
    corrected_words = tokenize_words(corrected.lower())
    corrected_dict = set(corrected_words)
    
    # T√¨m c√°c t·ª´ m·ªõi (kh√¥ng c√≥ trong t·ª´ ƒëi·ªÉn g·ªëc)
    new_words = corrected_dict - original_dict
    
    # Ch·ªâ t√≠nh ch·ªØ c√°i, lo·∫°i b·ªè s·ªë
    new_words_alpha_only = [word for word in new_words if word.isalpha()]
    
    # QUAN TR·ªåNG: L·ªçc c√°c t·ª´ m·ªõi h·ª£p l·ªá
    # 1. Lo·∫°i b·ªè k√Ω t·ª± ƒë∆°n l·∫ª n·∫øu l√† ph·∫ßn c·ªßa t·ª´ vi·∫øt t·∫Øt: "ESG" ‚Üí "E.S.G" ‚Üí "e","s","g"
    # 2. Cho ph√©p m·ªü r·ªông t·ª´ vi·∫øt t·∫Øt: "CP" ‚Üí "C·ªï Ph·∫ßn" (n·∫øu C v√† P n·∫±m trong ƒë·∫ßu c√°c t·ª´ m·ªõi)
    original_lower = original.lower()
    new_words_filtered = []
    
    # T√¨m c√°c t·ª´ vi·∫øt t·∫Øt trong original (t·ª´ c√≥ 2-5 k√Ω t·ª±, to√†n ch·ªØ hoa trong b·∫£n g·ªëc)
    acronyms_in_original = set()
    for orig_word in tokenize_words(original):  # Kh√¥ng lowercase ·ªü ƒë√¢y
        if 2 <= len(orig_word) <= 5 and orig_word.isupper():
            acronyms_in_original.add(orig_word.lower())
    
    for word in new_words_alpha_only:
        if len(word) == 1:
            # K√Ω t·ª± ƒë∆°n l·∫ª - ki·ªÉm tra xem c√≥ ph·∫£i l√† ph·∫ßn c·ªßa t·ª´ vi·∫øt t·∫Øt kh√¥ng
            is_part_of_acronym = any(word in orig_word for orig_word in original_dict if len(orig_word) > 1)
            if not is_part_of_acronym:
                new_words_filtered.append(word)
        else:
            # T·ª´ c√≥ 2+ k√Ω t·ª± - ki·ªÉm tra xem c√≥ ph·∫£i l√† ph·∫ßn m·ªü r·ªông c·ªßa t·ª´ vi·∫øt t·∫Øt kh√¥ng
            # V√≠ d·ª•: "c·ªï", "ph·∫ßn" c√≥ th·ªÉ l√† m·ªü r·ªông c·ªßa "cp" (c·ªï ph·∫ßn)
            is_expansion_of_acronym = False
            for acronym in acronyms_in_original:
                # L·∫•y ch·ªØ c√°i ƒë·∫ßu c·ªßa c√°c t·ª´ m·ªõi
                first_letters = ''.join([w[0] for w in new_words_alpha_only if len(w) > 1])
                if acronym in first_letters or first_letters.startswith(acronym):
                    is_expansion_of_acronym = True
                    break
            
            if not is_expansion_of_acronym:
                new_words_filtered.append(word)
    
    new_words_alpha_only = new_words_filtered
    
    # Ph√°t hi·ªán t·ª´ ti·∫øng Anh
    english_words = detect_english_words(corrected)
    
    return {
        'original_vocab_size': len(original_dict),
        'corrected_vocab_size': len(corrected_dict),
        'new_words_count': len(new_words_alpha_only),  # Ch·ªâ ƒë·∫øm ch·ªØ c√°i
        'new_words': sorted(list(new_words_alpha_only)),  # Ch·ªâ tr·∫£ v·ªÅ ch·ªØ c√°i
        'all_words_in_dict': len(new_words_alpha_only) == 0,  # Ch·ªâ ki·ªÉm tra ch·ªØ c√°i
        'english_words': english_words
    }

def compare_texts(original: str, corrected: str) -> dict:
    """
    So s√°nh 2 vƒÉn b·∫£n v√† t√¨m c√°c t·ª´ kh√°c nhau s·ª≠ d·ª•ng difflib
    KH√îNG PH√ÇN BI·ªÜT CH·ªÆ HOA CH·ªÆ TH∆Ø·ªúNG
    
    Returns:
        dict: Th√¥ng tin chi ti·∫øt v·ªÅ s·ª± kh√°c bi·ªát
    """
    from difflib import SequenceMatcher
    
    # Chuy·ªÉn t·∫•t c·∫£ v·ªÅ lowercase ƒë·ªÉ so s√°nh
    original_words = tokenize_words(original.lower())
    corrected_words = tokenize_words(corrected.lower())
    
    # S·ª≠ d·ª•ng SequenceMatcher ƒë·ªÉ t√¨m s·ª± kh√°c bi·ªát
    matcher = SequenceMatcher(None, original_words, corrected_words)
    differences = []
    
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'replace':
            # T·ª´ b·ªã thay th·∫ø
            for idx in range(max(i2-i1, j2-j1)):
                orig = original_words[i1+idx] if i1+idx < i2 else None
                corr = corrected_words[j1+idx] if j1+idx < j2 else None
                if orig and corr:
                    differences.append(('replace', orig, corr, i1+idx+1))
                elif orig:
                    differences.append(('delete', orig, '', i1+idx+1))
                elif corr:
                    differences.append(('insert', '', corr, i1+idx+1))
        elif tag == 'delete':
            # T·ª´ b·ªã x√≥a
            for idx in range(i1, i2):
                differences.append(('delete', original_words[idx], '', idx+1))
        elif tag == 'insert':
            # T·ª´ ƒë∆∞·ª£c th√™m v√†o
            for idx in range(j1, j2):
                differences.append(('insert', '', corrected_words[idx], i1+1))
    
    return {
        'total_words_original': len(original_words),
        'total_words_corrected': len(corrected_words),
        'different_words_count': len(differences),
        'differences': differences
    }

# helpers = Helpers()
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(script_dir))
input_folder = os.path.join(project_root, "src", "data", "contents")
output_folder = os.path.join(project_root, "src", "data", "grammar")

# Note: We now read files directly in the main loop, so no need for pages dictionary
# pages = read_all_txt_list(input_folder)
# pages = [' '.join(helpers.bm25_preprocessing_func(page)) for page in pages]
# pages = [page.replace("B√°o c√°o ph√°t tri·ªÉn b·ªÅn v·ªØng", " ") for page in pages if page]

os.makedirs(output_folder, exist_ok=True)

# X√°c ƒë·ªãnh th∆∞ m·ª•c g·ªëc c·ªßa d·ª± √°n
project_root = Path(__file__).resolve().parent.parent.parent

# Th∆∞ m·ª•c ch·ª©a file contents
contents_dir = project_root / "src" / "data" / "contents"

# Th∆∞ m·ª•c output
output_folder = project_root / "src" / "data" / "grammar"

# V·ªä TR√ç B·∫ÆT ƒê·∫¶U X·ª¨ L√ù
START_PAGE = 1  # Thay ƒë·ªïi s·ªë n√†y ƒë·ªÉ b·∫Øt ƒë·∫ßu t·ª´ trang kh√°c

print(model_name)
try:
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        dtype=torch.bfloat16,          # S·ª≠ d·ª•ng bfloat16 ƒë·ªÉ tƒÉng t·ªëc v√† ti·∫øt ki·ªám VRAM
        device_map="auto",
        trust_remote_code=True,
        # attn_implementation="flash_attention_2",  # ‚ö°Ô∏è TƒÉng t·ªëc attention
    ).eval()                                 # Ch·ªâ d√πng cho inference
    model = torch.compile(model, mode="reduce-overhead")             # TƒÉng t·ªëc n·∫øu PyTorch >= 2.0
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
except Exception as e:
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,          # S·ª≠ d·ª•ng bfloat16 ƒë·ªÉ tƒÉng t·ªëc v√† ti·∫øt ki·ªám VRAM
        device_map="auto",
        trust_remote_code=True,
        # attn_implementation="flash_attention_2",  # ‚ö°Ô∏è TƒÉng t·ªëc attention
    ).eval()                                 # Ch·ªâ d√πng cho inference
    model = torch.compile(model, mode="reduce-overhead")             # TƒÉng t·ªëc n·∫øu PyTorch >= 2.0
    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)

# üß† H√†m hi·ªáu ƒë√≠nh ch√≠nh t·∫£
def correct_vietnamese(text: str, repeat_reminder: int, model, tokenizer, use_enhanced_prompt: bool = False, memory_examples: list | None = None) -> str:
    """
    Hi·ªáu ƒë√≠nh ch√≠nh t·∫£ ti·∫øng Vi·ªát v·ªõi kh·∫£ nƒÉng h·ªçc t·ª´ c√°c v√≠ d·ª• tr∆∞·ªõc ƒë√≥
    
    Args:
        text: VƒÉn b·∫£n c·∫ßn hi·ªáu ƒë√≠nh
        repeat_reminder: S·ªë l·∫ßn retry (kh√¥ng s·ª≠ d·ª•ng trong logic hi·ªán t·∫°i)
        model: Model LLM
        tokenizer: Tokenizer
        use_enhanced_prompt: N·∫øu True, s·ª≠ d·ª•ng prompt n√¢ng cao v·ªõi ƒë·ªãnh d·∫°ng chi ti·∫øt h∆°n
        memory_examples: List of (original, corrected, similarity) tuples - C√°c v√≠ d·ª• t·ªët nh·∫•t ƒë·ªÉ model tham kh·∫£o
    
    Returns:
        str: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c hi·ªáu ƒë√≠nh
    """
    prompt = f"\n\n{text}"
    
    # Prompt c∆° b·∫£n - ƒë∆°n gi·∫£n, √≠t r√†ng bu·ªôc
    basic_prompt = """You are an **intelligent Vietnamese spelling and grammar corrector and formatter**.

**Your tasks**:

* Check and **correct all Vietnamese spelling, punctuation, and grammar errors** with perfect linguistic accuracy.
* **Preserve the original meaning, structure, and formatting** ‚Äî do **not** summarize, interpret, or add/remove content.
* If the text contains **multiple semantic blocks**, separate them using the delimiter `\\n\\n`.
* If the text includes **table-like or list-like data**, present it clearly as a **flat ordered list**, in this format:
  Item 1: [T√™n] ‚Äì [M√¥ t·∫£ + d·ªØ li·ªáu ch√≠nh]
  Item 2: [T√™n] ‚Äì [M√¥ t·∫£ + d·ªØ li·ªáu ch√≠nh]
* If the text includes **labels, headers, or hierarchical sections** (v√≠ d·ª•: "ƒê·∫∑c bi·ªát tr·ªçng y·∫øu", "R·∫•t tr·ªçng y·∫øu", "Tr·ªçng y·∫øu", ...), **preserve them exactly as input** and keep their associated items grouped below.
* If the text describes a **chart, process, or workflow**, express it as a **logical sequence** using arrows (>>) between steps.
* Maintain correct **Vietnamese capitalization, punctuation, spacing, and diacritics**.
* Preserve all **proper nouns, abbreviations, organizational names, and references** exactly (e.g. E.S.G, CP, CT, ESG, CK, BIWASE, ...).
* Ensure **consistent line breaks and spacing** as in the input, especially for headings, sections, and chapter titles.
* **No English response**, **Just Vietnamese**.

**Output requirements:**

* Output **only the corrected text**, with the same formatting, line breaks, and structure as the input.
* Do **not** include explanations, comments, or any extra symbols (no markdown, no bullets unless already in text).
* The output must look like a polished, publication-ready Vietnamese document while retaining the original structure and flow.
"""
    
    # Prompt n√¢ng cao - chi ti·∫øt h∆°n, ƒë·ªãnh d·∫°ng r√µ r√†ng
    enhanced_prompt = """You are an **intelligent Vietnamese spelling and grammar corrector**.

**Your tasks**:

* Check and **correct all Vietnamese spelling, punctuation, and grammar errors**.
* **Preserve the original meaning, character count, and formatting** ‚Äî do not add, remove, or summarize content.
* If the text contains **multiple semantic blocks**, separate them using the delimiter `\\n\\n`.
* If the text includes **table-like or list-like data**, present it clearly as a **flat ordered list** (kh√¥ng chia c·ªôt, kh√¥ng nh√≥m).
  V√≠ d·ª•:

  1. [T√™n ch·ªß ƒë·ªÅ] ‚Äì [M√¥ t·∫£/ng·ªØ c·∫£nh n·∫øu c√≥]
  2. [T√™n ch·ªß ƒë·ªÅ] ‚Äì [M√¥ t·∫£/ng·ªØ c·∫£nh n·∫øu c√≥]
* If the text includes **labels or axes** (v√≠ d·ª•: "R·∫•t tr·ªçng y·∫øu", "Tr·ªçng y·∫øu", ...), h√£y gi·ªØ nguy√™n ch√∫ng nh∆∞ ti√™u ƒë·ªÅ d√≤ng, kh√¥ng s·∫Øp x·∫øp l·∫°i.
* If the text describes a **chart, process, or workflow**, tr√¨nh b√†y l·∫°i th√†nh **tr√¨nh t·ª± logic** b·∫±ng m≈©i t√™n (>>) gi·ªØa c√°c b∆∞·ªõc.
* Maintain correct **Vietnamese capitalization, punctuation, and spacing**.
* Do not change proper nouns, organizational names, or abbreviations (v√≠ d·ª•: E.S.G, CP, CT, ESG, CK, ...).
* **No English response**, **Just Vietnamese**.

**Output requirements:**

* Ch·ªâ xu·∫•t ra **vƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c ch·ªânh ch√≠nh t·∫£ v√† ng·ªØ ph√°p**, kh√¥ng c√≥ gi·∫£i th√≠ch, kh√¥ng th√™m k√Ω hi·ªáu ƒë·ªãnh d·∫°ng.
* **Gi·ªØ nguy√™n b·ªë c·ª•c nh∆∞ ƒë·∫ßu v√†o** (bao g·ªìm ti√™u ƒë·ªÅ, danh s√°ch, m·ª•c l·ª•c ch∆∞∆°ng, v√† c·∫•u tr√∫c ph√¢n c·∫•p).
"""
    
    # Ch·ªçn prompt ph√π h·ª£p
    system_prompt = enhanced_prompt if use_enhanced_prompt else basic_prompt
    
    messages = [
        {
            "role": "system",
            "content": system_prompt,
        }
    ]
    
    # üíæ TH√äM B·ªò NH·ªö: N·∫øu c√≥ v√≠ d·ª• t·ªët, th√™m v√†o messages ƒë·ªÉ model h·ªçc
    if memory_examples and len(memory_examples) > 0:
        print(f"  üß† S·ª≠ d·ª•ng {len(memory_examples)} v√≠ d·ª• t·ªët nh·∫•t l√†m reference")
        for idx, (orig_text, corrected_text, sim_score) in enumerate(memory_examples, 1):
            messages.append({
                "role": "user",
                "content": f"V√≠ d·ª• {idx} (similarity: {sim_score:.3f}):\n\n{orig_text}"
            })
            messages.append({
                "role": "assistant",
                "content": corrected_text
            })
    
    # Th√™m vƒÉn b·∫£n hi·ªán t·∫°i c·∫ßn x·ª≠ l√Ω
    messages.append({"role": "user", "content": prompt})

    total_len = len(text.split())
    print(f"Total input tokens: {total_len}")
    print(f"üìù Using {'enhanced' if use_enhanced_prompt else 'basic'} prompt")

    max_new_tokens = 2048
        
    inp = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer([inp], return_tensors="pt").to(model.device)

    with torch.inference_mode():
        out_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            eos_token_id=tokenizer.eos_token_id,
        )

    gen = out_ids[0][inputs.input_ids.shape[1]:]
    result = tokenizer.decode(gen, skip_special_tokens=True).strip()
    return result

def extract_page_number(filename):
    """Tr√≠ch xu·∫•t s·ªë trang t·ª´ t√™n file.

    H·ªó tr·ª£ c√°c d·∫°ng:
    - "page_1" -> 1
    - "page_cleared_2" -> 2
    - B·∫•t k·ª≥ t√™n n√†o k·∫øt th√∫c b·∫±ng s·ªë -> l·∫•y nh√≥m s·ªë cu·ªëi c√πng
    """
    m = re.search(r"(\d+)$", filename)
    if m:
        try:
            return int(m.group(1))
        except ValueError:
            return None
    return None

# Thu th·∫≠p t·∫•t c·∫£ file text t·ª´ contents (kh√¥ng c√≥ suffix _ocr)
content_files = {}
for content_file in contents_dir.glob("page_cleared_*.txt"):
    # B·ªè qua c√°c file c√≥ suffix _ocr
    if "_ocr" not in content_file.stem:
        page_num = extract_page_number(content_file.stem)
        if page_num is not None and page_num >= START_PAGE:
            content_files[page_num] = content_file

# Thu th·∫≠p t·∫•t c·∫£ file t·ª´ output_folder (grammar)
grammar_files = {}
for grammar_file in output_folder.glob("page_cleared_*.txt"):
    page_num = extract_page_number(grammar_file.stem)
    if page_num is not None and page_num >= START_PAGE:
        grammar_files[page_num] = grammar_file

# Hi·ªÉn th·ªã k·∫øt qu·∫£
print(f"T√¨m th·∫•y {len(content_files)} file contents (t·ª´ trang {START_PAGE})")
print(f"T√¨m th·∫•y {len(grammar_files)} file grammar (t·ª´ trang {START_PAGE})")

# Add project root to path for imports
import sys
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# from src.models.embedd import QwenEmbedding
from src.models.halong_embedd import HalongEmbedding as QwenEmbedding

embedding = QwenEmbedding()
array_similarity = []
array_word_changes = []

# üíæ B·ªò NH·ªö: L∆∞u 2 ph·∫£n h·ªìi t·ªët nh·∫•t ƒë·ªÉ model tham kh·∫£o
best_examples = []  # List of (original, corrected, similarity) tuples

# So kh·ªõp c√°c file theo page number
for page_num in sorted(content_files.keys()):
    content_path = content_files[page_num]
    grammar_path = grammar_files.get(page_num)
    
    # Ensure grammar_path exists
    if grammar_path is None:
        # ƒê·ªìng b·ªô v·ªõi pattern ƒë·ªçc "page_cleared_*.txt"
        grammar_path = output_folder / f"page_cleared_{page_num}.txt"
    
    print(f"\n{'='*70}")
    print(f"üìÑ Page {page_num}:")
    print(f"{'='*70}")
    print(f"  Content: {content_path}")
    print(f"  Grammar: {grammar_path}")
    
    # Read content from the actual file
    with open(content_path, 'r', encoding='utf-8') as f:
        page_content = f.read()
    
    # L·∫¶N 1: Th·ª≠ v·ªõi basic prompt (+ memory n·∫øu c√≥)
    corrected_text = correct_vietnamese(
        page_content, 
        page_num, 
        model, 
        tokenizer, 
        use_enhanced_prompt=False,
        memory_examples=best_examples  # Truy·ªÅn b·ªô nh·ªõ v√†o
    )
    similarity = embedding.calculate_similarity(page_content, corrected_text)
    
    if similarity < 0.8 and len(page_content.split()) < 20:
        print(f"‚ö†Ô∏è Trang {page_num}: Similarity th·∫•p ({similarity:.3f}) v√† vƒÉn b·∫£n ng·∫Øn (<20 t·ª´). B·ªè qua trang n√†y.")
        with open(grammar_path, 'w', encoding='utf-8') as f:
            f.write(page_content)
        continue

    # Ki·ªÉm tra t·ª´ ƒëi·ªÉn
    vocab_check = check_vocabulary_match(page_content, corrected_text)
    
    # ƒêI·ªÄU KI·ªÜN RETRY: similarity < 0.95 HO·∫∂C c√≥ nhi·ªÅu t·ª´ m·ªõi (> 5 t·ª´)
    max_retry = 1  # Ch·ªâ retry 1 l·∫ßn v·ªõi enhanced prompt
    retry_count = 0
    
    if (similarity < 0.95 or vocab_check['new_words_count'] > 5) and retry_count < max_retry:
        print(f"\nüîÑ RETRY v·ªõi enhanced prompt (similarity={similarity:.3f}, new_words={vocab_check['new_words_count']})")
        retry_count += 1
        
        # L·∫¶N 2: Th·ª≠ v·ªõi enhanced prompt (+ memory)
        corrected_text = correct_vietnamese(
            page_content, 
            page_num, 
            model, 
            tokenizer, 
            use_enhanced_prompt=True,
            memory_examples=best_examples  # Truy·ªÅn b·ªô nh·ªõ v√†o
        )
        similarity = embedding.calculate_similarity(page_content, corrected_text)
        vocab_check = check_vocabulary_match(page_content, corrected_text)
        
        print(f"‚úÖ Sau retry: similarity={similarity:.3f}, new_words={vocab_check['new_words_count']}")
    
    # üíæ C·∫¨P NH·∫¨T B·ªò NH·ªö: L∆∞u c√°c v√≠ d·ª• t·ªët nh·∫•t
    # Ch·ªâ l∆∞u n·∫øu similarity >= 0.95 v√† √≠t t·ª´ m·ªõi
    if similarity >= 0.95 and vocab_check['new_words_count'] <= 3:
        best_examples.append((page_content, corrected_text, similarity))
        # S·∫Øp x·∫øp theo similarity gi·∫£m d·∫ßn v√† ch·ªâ gi·ªØ 2 v√≠ d·ª• t·ªët nh·∫•t
        best_examples.sort(key=lambda x: x[2], reverse=True)
        best_examples = best_examples[:2]
        print(f"  üíæ ƒê√£ l∆∞u v√†o b·ªô nh·ªõ (t·ªïng: {len(best_examples)} v√≠ d·ª•, similarity: {similarity:.3f})")
    
    # So s√°nh chi ti·∫øt
    comparison = compare_texts(page_content, corrected_text)
    
    print(f"\nüìä K·∫øt qu·∫£ cu·ªëi c√πng:")
    print(f"  - Similarity: {similarity:.4f}")
    print(f"  - S·ªë t·ª´ g·ªëc: {comparison['total_words_original']}")
    print(f"  - S·ªë t·ª´ ƒë√£ s·ª≠a: {comparison['total_words_corrected']}")
    print(f"  - S·ªë t·ª´ kh√°c nhau: {comparison['different_words_count']}")
    print(f"  - T·ª´ m·ªõi (ch·ªØ c√°i): {vocab_check['new_words_count']} t·ª´")
    
    if vocab_check['new_words_count'] > 0:
        print(f"  - C√°c t·ª´ m·ªõi: {', '.join(vocab_check['new_words'][:10])}")
    
    if vocab_check['english_words']:
        print(f"  - ‚ö†Ô∏è Ph√°t hi·ªán {len(vocab_check['english_words'])} t·ª´ ti·∫øng Anh: {', '.join(vocab_check['english_words'][:5])}")
    
    if comparison['different_words_count'] > 0:
        print(f"\nüìù Chi ti·∫øt thay ƒë·ªïi:")
        for diff in comparison['differences'][:10]:
            print(f"    ‚Ä¢ {diff}")
    
    # L∆∞u k·∫øt qu·∫£
    with open(grammar_path, 'w', encoding='utf-8') as f:
        f.write(corrected_text)
    
    print(f"\n‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o: {grammar_path}")
    
    array_similarity.append({
        'page': page_num,
        'similarity': similarity,
        'retry_count': retry_count
    })
    
    array_word_changes.append({
        'page': page_num,
        'word_diff': comparison['different_words_count'],
        'new_words': vocab_check['new_words_count']
    })

# T·ªïng k·∫øt
print(f"\n{'='*70}")
print(f"üìà T·ªîNG K·∫æT:")
print(f"{'='*70}")
print(f"T·ªïng s·ªë trang ƒë√£ x·ª≠ l√Ω: {len(array_similarity)}")
avg_similarity = sum([x['similarity'] for x in array_similarity]) / len(array_similarity) if array_similarity else 0
print(f"Similarity trung b√¨nh: {avg_similarity:.4f}")
retry_pages = [x for x in array_similarity if x['retry_count'] > 0]
print(f"S·ªë trang c·∫ßn retry: {len(retry_pages)}")

if retry_pages:
    print(f"\nC√°c trang ƒë√£ retry:")
    for page_info in retry_pages:
        print(f"  - Page {page_info['page']}: similarity={page_info['similarity']:.3f}")
