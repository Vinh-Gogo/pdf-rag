import os
import sys
from pathlib import Path
from typing import List, Dict
from langchain_openai import OpenAIEmbeddings
from langchain_qdrant import QdrantVectorStore
from pydantic import SecretStr
import json
import uuid
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Add project root to path
script_dir = Path(__file__).resolve().parent
project_root = script_dir.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from src.helpers.init_qdrant import qdrant_client

def read_pages_from_directory(input_dir: str) -> List[Dict[str, str]]:
    """
    Äá»c táº¥t cáº£ file txt trong thÆ° má»¥c vÃ  táº¡o list pages (má»—i file lÃ  má»™t page)

    Args:
        input_dir (str): ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c chá»©a file txt

    Returns:
        List[Dict[str, str]]: List cÃ¡c pages vá»›i metadata
    """
    input_path = Path(input_dir)
    pages = []

    # Láº¥y táº¥t cáº£ file .txt cÃ³ Ä‘á»‹nh dáº¡ng page_NUMBER.txt
    txt_files = list(input_path.glob("page_*.txt"))
    
    # Sáº¯p xáº¿p pages theo sá»‘ thá»© tá»± tá»« tÃªn file
    def extract_page_num(file_path):
        try:
            return int(file_path.stem.split('_')[1])
        except (IndexError, ValueError):
            return 0
    
    txt_files = sorted(txt_files, key=extract_page_num)

    print(f"ğŸ“‚ TÃ¬m tháº¥y {len(txt_files)} file txt trong {input_dir}")

    for file_path in txt_files:
        file_name = file_path.name
        page_num = extract_page_num(file_path)
        
        if page_num % 20 == 0 or page_num == 1:
            print(f"ğŸ“„ Äang Ä‘á»c: {file_name}")

        try:
            # Äá»c toÃ n bá»™ ná»™i dung file nhÆ° má»™t page
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()

            if content:  # Chá»‰ thÃªm náº¿u cÃ³ ná»™i dung
                # Äáº¿m sá»‘ paragraphs (tÃ¡ch bá»Ÿi \n\n)
                paragraphs = [p.strip() for p in content.split("\n\n") if p.strip()]
                
                page = {
                    'page_index': page_num,
                    'seq': len(paragraphs),
                    'content': content,
                    'word_count': len(content.split())
                }
                pages.append(page)

        except Exception as e:
            print(f"   âŒ Lá»—i Ä‘á»c file {file_name}: {e}")

    print(f"âœ… ÄÃ£ Ä‘á»c thÃ nh cÃ´ng {len(pages)} pages")
    return pages

def save_pages_to_json(pages: List[Dict[str, str]], output_file: str):
    """
    LÆ°u pages vÃ o file JSON

    Args:
        pages (List[Dict[str, str]]): List pages
        output_file (str): ÄÆ°á»ng dáº«n file output
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(pages, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ ÄÃ£ lÆ°u {len(pages)} pages vÃ o {output_file}")

def store_pages_in_qdrant_direct(pages: List[Dict[str, str]], collection_name: str = "esg_pages") -> QdrantVectorStore:
    """
    LÆ°u trá»¯ pages trá»±c tiáº¿p vÃ o Qdrant vá»›i metadata Ä‘áº§y Ä‘á»§

    Args:
        pages (List[Dict[str, str]]): List pages vá»›i metadata
        collection_name (str): TÃªn collection trong Qdrant

    Returns:
        QdrantVectorStore: Vector store Ä‘Ã£ táº¡o
    """
    from qdrant_client.models import PointStruct, VectorParams, Distance

    print(f"ğŸ”§ Äang táº¡o vector store trá»±c tiáº¿p cho {len(pages)} pages...")

    # Khá»Ÿi táº¡o embeddings
    model = str(os.getenv("OPENAI_API_MODEL_NAME_EMBED"))
    base_url = os.getenv("OPENAI_BASE_URL_EMBED")
    api_key = str(os.getenv("OPENAI_API_KEY_EMBED"))

    embeddings = OpenAIEmbeddings(
        model=model,
        base_url=base_url,
        api_key=SecretStr(api_key),
        tiktoken_enabled=False,
    )

    # XÃ³a collection cÅ© náº¿u tá»“n táº¡i
    try:
        qdrant_client.delete_collection(collection_name)
        print(f"ğŸ—‘ï¸ ÄÃ£ xÃ³a collection cÅ© '{collection_name}'")
    except:
        pass

    # Táº¡o collection má»›i vá»›i vector size phÃ¹ há»£p
    sample_embedding = embeddings.embed_query("test")
    vector_size = len(sample_embedding)

    qdrant_client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
    )
    print(f"ğŸ“ ÄÃ£ táº¡o collection má»›i '{collection_name}' vá»›i vector size {vector_size}")

    # Táº¡o points vá»›i payload Ä‘áº§y Ä‘á»§
    points = []
    for i, page in enumerate(pages):
        if i % 10 == 0:  # Hiá»ƒn thá»‹ tiáº¿n trÃ¬nh má»—i 10 pages
            print(f"ğŸ“Š Äang xá»­ lÃ½ page {i+1}/{len(pages)}...")

        # Táº¡o embedding cho toÃ n bá»™ ná»™i dung page
        vector = embeddings.embed_query(page['content'])

        # Táº¡o UUID cho point ID
        point_id = str(uuid.uuid4())

        # Táº¡o payload vá»›i táº¥t cáº£ metadata
        payload = {
            'page_index': page['page_index'],
            'content': page['content'],
            'seq': page['seq'],
            'word_count': page['word_count'],
            'page_content': page['content']  # Äá»ƒ tÆ°Æ¡ng thÃ­ch vá»›i LangChain
        }

        # Táº¡o point
        point = PointStruct(
            id=point_id,  # Sá»­ dá»¥ng UUID lÃ m ID
            vector=vector,
            payload=payload
        )
        points.append(point)

    # Upload points theo batch
    batch_size = 50  # Batch nhá» hÆ¡n vÃ¬ má»—i page cÃ³ thá»ƒ lá»›n
    for i in range(0, len(points), batch_size):
        batch = points[i:i+batch_size]
        qdrant_client.upsert(
            collection_name=collection_name,
            points=batch
        )
        print(f"ğŸ“¤ ÄÃ£ upload batch {i//batch_size + 1}/{(len(points)-1)//batch_size + 1}")

    print(f"âœ… ÄÃ£ táº¡o vector store '{collection_name}' vá»›i {len(points)} pages")

    # Táº¡o QdrantVectorStore wrapper Ä‘á»ƒ sá»­ dá»¥ng vá»›i LangChain
    vectorstore = QdrantVectorStore.from_existing_collection(
        embedding=embeddings,
        collection_name=collection_name,
        url=os.getenv("QDRANT_URL"),
        api_key=os.getenv("QDRANT_API_KEY"),
        prefer_grpc=True
    )

    return vectorstore

def retrieve_similar_pages(query: str, vectorstore, top_k: int = 5):
    """
    Tá»« cÃ¢u há»i, retrieval cÃ¡c pages cÃ³ ná»™i dung tÆ°Æ¡ng Ä‘á»“ng nháº¥t

    Args:
        query (str): CÃ¢u há»i cáº§n tÃ¬m
        vectorstore: QdrantVectorStore Ä‘Ã£ Ä‘Æ°á»£c táº¡o
        top_k (int): Sá»‘ lÆ°á»£ng káº¿t quáº£ tráº£ vá»

    Returns:
        List[Dict]: Danh sÃ¡ch káº¿t quáº£ vá»›i id, content, vÃ  score
    """
    print(f"ğŸ” Äang tÃ¬m kiáº¿m cho query: '{query}'")

    # Khá»Ÿi táº¡o embeddings
    model = str(os.getenv("OPENAI_API_MODEL_NAME_EMBED"))
    base_url = os.getenv("OPENAI_BASE_URL_EMBED")
    api_key = str(os.getenv("OPENAI_API_KEY_EMBED"))

    embeddings = OpenAIEmbeddings(
        model=model,
        base_url=base_url,
        api_key=SecretStr(api_key),
        tiktoken_enabled=False,
    )

    # Táº¡o embedding cho query
    query_vector = embeddings.embed_query(query)

    # TÃ¬m kiáº¿m trá»±c tiáº¿p tá»« Qdrant
    search_results = qdrant_client.search(
        collection_name="esg_pages",
        query_vector=query_vector,
        limit=top_k,
        with_payload=True,
        with_vectors=False
    )

    # Format káº¿t quáº£
    formatted_results = []
    for result in search_results:
        payload = result.payload or {}
        formatted_results.append({
            'page_index': payload.get('page_index', 'N/A'),
            'content': payload.get('content', ''),
            'seq': payload.get('seq', 'N/A'),
            'word_count': payload.get('word_count', 'N/A'),
            'similarity_score': float(result.score)
        })

    return formatted_results

def display_page_retrieval_results(results: List[Dict]):
    """
    Hiá»ƒn thá»‹ káº¿t quáº£ retrieval cho pages

    Args:
        results (List[Dict]): Káº¿t quáº£ tá»« retrieve_similar_pages
    """
    print(f"\nğŸ“‹ Káº¾T QUáº¢ RETRIEVAL top-k: {len(results)}")
    print("=" * 10)

    for i, result in enumerate(results, 1):
        print(f"\n{i}. [SEARCH] á» Page: {result['page_index']}")
        print(f"   Sequences: {result['seq']}")
        print(f"   Words: {result['word_count']}")
        print(f"   Similarity Score: {result['similarity_score']:.4f}")
        print(f"   Content: \n{result['content'][:200]}{'...' if len(result['content']) > 200 else ''}")

    print("\n" + "=" * 80)

# ========== PIPELINE ==========

if __name__ == "__main__":
    # Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n
    script_dir = Path(__file__).resolve().parent
    project_root = script_dir.parent.parent
    input_dir = project_root / "src" / "data" / "results" / "grammar"
    output_dir = project_root / "src" / "store" / "data_to_push"
    output_dir.mkdir(parents=True, exist_ok=True)

    print("ğŸš€ Báº®T Äáº¦U Xá»¬ LÃ PAGES")
    print(f"ğŸ“‚ Input:  {input_dir}")
    print(f"ğŸ“‚ Output: {output_dir}")

    # Äá»c vÃ  xá»­ lÃ½ pages
    pages = read_pages_from_directory(str(input_dir))

    # Hiá»ƒn thá»‹ thá»‘ng kÃª
    print(f"\nğŸ“Š THá»NG KÃŠ:")
    print(f"   Tá»•ng sá»‘ pages: {len(pages)}")
    total_words = sum(int(page['word_count']) for page in pages)
    print(f"   Tá»•ng tá»«: {total_words:,}")

    # Thá»‘ng kÃª chi tiáº¿t
    print(f"\nğŸ“‹ CHI TIáº¾T THEO PAGE:")
    for page in pages:
        print(f"   Page {page['page_index']}: {page['word_count']:,} words, {page['seq']} sequences")

    # LÆ°u káº¿t quáº£ JSON
    output_file = output_dir / "pages_data.json"
    save_pages_to_json(pages, str(output_file))

    # LÆ°u vÃ o Qdrant
    try:
        vectorstore = store_pages_in_qdrant_direct(pages, "esg_pages")
        print(f"\nâœ… ÄÃ£ lÆ°u trá»¯ thÃ nh cÃ´ng vÃ o Qdrant!")

        # Test retrieval
        print(f"\nğŸ§ª TEST RETRIEVAL:")
        test_queries = [
            "thá»ƒ dá»¥c thá»ƒ thao sá»©c khá»e dá»“i dÃ o",
            "Vá»‘n Ä‘iá»u lá»‡",
            "quáº£n lÃ½ rá»§i ro",
            "doanh thu nÄƒm 2024"
        ]

        for query in test_queries:
            results = retrieve_similar_pages(query, vectorstore, top_k=3)
            display_page_retrieval_results(results)
            print("\n" + "-"*50)

    except Exception as e:
        print(f"\nâŒ Lá»—i lÆ°u trá»¯ vÃ o Qdrant: {e}")
        print("Vui lÃ²ng kiá»ƒm tra OPENAI_API_KEY_EMBED trong .env file")
    
    # Hiá»ƒn thá»‹ vÃ i examples
    print(f"\nğŸ“ VÃ Dá»¤ PAGES:")
    for i, page in enumerate(pages[:3], 1):
        print(f"\n{i}. á» trang {page['page_index']}")
        print(f"   Words: {page['word_count']:,}")
        print(f"   Sequences: {page['seq']}")
        print(f"   Content: \n{page['content'][:150]}{'...' if len(page['content']) > 150 else ''}")

    print(f"\nâœ… HOÃ€N THÃ€NH! ÄÃ£ xá»­ lÃ½ {len(pages)} pages tá»« thÆ° má»¥c {input_dir}")