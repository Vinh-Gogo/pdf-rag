"""
Sequences to Vector Store Pipeline

Quy tr√¨nh:
1. ƒê·ªçc t·∫•t c·∫£ file text t·ª´ src/data/raw
2. T√°ch m·ªói file th√†nh c√°c sequences (ƒëo·∫°n vƒÉn)
3. L∆∞u t·∫•t c·∫£ sequences v√†o Qdrant Vector Store v·ªõi metadata
4. Test retrieval
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Optional
from langchain_openai import OpenAIEmbeddings
from langchain_qdrant import QdrantVectorStore
from pydantic import SecretStr
import json
import uuid
import re

# Add project root to path
script_dir = Path(__file__).resolve().parent
project_root = script_dir.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from src.helpers.init_qdrant import qdrant_client


def extract_page_number(filename: str) -> int:
    """
    Tr√≠ch xu·∫•t s·ªë trang t·ª´ t√™n file (page_1.txt -> 1)
    
    Args:
        filename (str): T√™n file
        
    Returns:
        int: S·ªë trang
    """
    try:
        match = re.search(r'page_(\d+)', filename)
        if match:
            return int(match.group(1))
        return 0
    except:
        return 0


def read_sequences_from_directory(input_dir: str, min_words: int = 10) -> List[Dict]:
    """
    ƒê·ªçc t·∫•t c·∫£ file txt v√† t√°ch th√†nh sequences (ƒëo·∫°n vƒÉn)
    
    Args:
        input_dir (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a file txt
        min_words (int): S·ªë t·ª´ t·ªëi thi·ªÉu cho m·ªôt sequence
        
    Returns:
        List[Dict]: List c√°c sequences v·ªõi metadata
    """
    input_path = Path(input_dir)
    all_sequences = []
    
    # L·∫•y t·∫•t c·∫£ file .txt c√≥ ƒë·ªãnh d·∫°ng page_NUMBER.txt
    txt_files = list(input_path.glob("page_*.txt"))
    
    # S·∫Øp x·∫øp theo s·ªë trang
    txt_files = sorted(txt_files, key=lambda x: extract_page_number(x.name))
    
    print(f"üìÇ T√¨m th·∫•y {len(txt_files)} file txt trong {input_dir}")
    print(f"üîç S·∫Ω t√°ch c√°c sequences v·ªõi t·ªëi thi·ªÉu {min_words} t·ª´\n")
    
    total_sequences = 0
    
    for file_path in txt_files:
        page_num = extract_page_number(file_path.name)
        
        try:
            # ƒê·ªçc n·ªôi dung file
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            if not content:
                continue
            
            # T√°ch th√†nh sequences b·∫±ng \n\n (double newline)
            raw_sequences = content.split('\n\n')
            
            # Filter v√† clean sequences
            page_sequences = []
            for seq_idx, seq_text in enumerate(raw_sequences, 1):
                seq_text = seq_text.strip()
                
                if not seq_text:
                    continue
                
                # ƒê·∫øm s·ªë t·ª´
                word_count = len(seq_text.split())
                
                # Ch·ªâ gi·ªØ sequences c√≥ ƒë·ªß s·ªë t·ª´
                if word_count >= min_words:
                    sequence = {
                        'page_index': page_num,
                        'seq_index': seq_idx,
                        'seq_id': f"page_{page_num}_seq_{seq_idx}",
                        'content': seq_text,
                        'word_count': word_count,
                        'char_count': len(seq_text)
                    }
                    page_sequences.append(sequence)
            
            all_sequences.extend(page_sequences)
            total_sequences += len(page_sequences)
            
            if page_num % 20 == 0 or page_num == 1:
                print(f"üìÑ Page {page_num}: {len(page_sequences)} sequences")
        
        except Exception as e:
            print(f"‚ùå L·ªói ƒë·ªçc file {file_path.name}: {e}")
    
    print(f"\n‚úÖ ƒê√£ t√°ch th√†nh c√¥ng {total_sequences} sequences t·ª´ {len(txt_files)} pages")
    return all_sequences


def save_sequences_to_json(sequences: List[Dict], output_file: str):
    """
    L∆∞u sequences v√†o file JSON
    
    Args:
        sequences (List[Dict]): List sequences
        output_file (str): ƒê∆∞·ªùng d·∫´n file output
    """
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(sequences, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ ƒê√£ l∆∞u {len(sequences)} sequences v√†o {output_file}")


def store_sequences_in_qdrant(
    sequences: List[Dict], 
    collection_name: str = "esg_sequences",
    batch_size: int = 50
) -> QdrantVectorStore:
    """
    L∆∞u tr·ªØ sequences v√†o Qdrant v·ªõi metadata ƒë·∫ßy ƒë·ªß
    
    Args:
        sequences (List[Dict]): List sequences v·ªõi metadata
        collection_name (str): T√™n collection trong Qdrant
        batch_size (int): S·ªë sequences m·ªói batch
        
    Returns:
        QdrantVectorStore: Vector store ƒë√£ t·∫°o
    """
    from qdrant_client.models import PointStruct, VectorParams, Distance
    
    print(f"\nüîß ƒêang t·∫°o vector store cho {len(sequences)} sequences...")
    
    # Kh·ªüi t·∫°o embeddings
    model = str(os.getenv("OPENAI_API_MODEL_NAME_EMBED"))
    base_url = os.getenv("OPENAI_BASE_URL_EMBED")
    api_key = str(os.getenv("OPENAI_API_KEY_EMBED"))
    
    embeddings = OpenAIEmbeddings(
        model=model,
        base_url=base_url,
        api_key=SecretStr(api_key),
        tiktoken_enabled=False,
    )
    
    # X√≥a collection c≈© n·∫øu t·ªìn t·∫°i
    try:
        qdrant_client.delete_collection(collection_name)
        print(f"üóëÔ∏è ƒê√£ x√≥a collection c≈© '{collection_name}'")
    except:
        pass
    
    # T·∫°o collection m·ªõi
    sample_embedding = embeddings.embed_query("test")
    vector_size = len(sample_embedding)
    
    qdrant_client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
    )
    print(f"üìÅ ƒê√£ t·∫°o collection '{collection_name}' v·ªõi vector size {vector_size}")
    
    # T·∫°o points v·ªõi embedding
    print(f"\nüìä ƒêang t·∫°o embeddings cho {len(sequences)} sequences...")
    points = []
    
    for i, seq in enumerate(sequences):
        # Hi·ªÉn th·ªã ti·∫øn tr√¨nh
        if (i + 1) % 20 == 0 or i == 0:
            print(f"   üîÑ Processing: {i+1}/{len(sequences)} sequences...")
        
        # T·∫°o embedding cho sequence
        vector = embeddings.embed_query(seq['content'])
        
        # T·∫°o point
        point = PointStruct(
            id=str(uuid.uuid4()),
            vector=vector,
            payload={
                'page_index': seq['page_index'],
                'seq_index': seq['seq_index'],
                'seq_id': seq['seq_id'],
                'content': seq['content'],
                'word_count': seq['word_count'],
                'char_count': seq['char_count'],
                'page_content': seq['content']  # ƒê·ªÉ t∆∞∆°ng th√≠ch v·ªõi LangChain
            }
        )
        points.append(point)
    
    # Upload points theo batch
    print(f"\nüì§ ƒêang upload {len(points)} points v√†o Qdrant...")
    total_batches = (len(points) - 1) // batch_size + 1
    
    for i in range(0, len(points), batch_size):
        batch = points[i:i+batch_size]
        qdrant_client.upsert(
            collection_name=collection_name,
            points=batch
        )
        batch_num = i // batch_size + 1
        print(f"   ‚úÖ Uploaded batch {batch_num}/{total_batches}")
    
    print(f"\n‚úÖ ƒê√£ t·∫°o vector store '{collection_name}' v·ªõi {len(points)} sequences")
    
    # T·∫°o QdrantVectorStore wrapper
    vectorstore = QdrantVectorStore.from_existing_collection(
        embedding=embeddings,
        collection_name=collection_name,
        url=os.getenv("QDRANT_URL"),
        api_key=os.getenv("QDRANT_API_KEY"),
        prefer_grpc=True
    )
    
    return vectorstore


def retrieve_similar_sequences(
    query: str, 
    collection_name: str = "esg_sequences",
    top_k: int = 5
) -> List[Dict]:
    """
    T√¨m ki·∫øm sequences t∆∞∆°ng t·ª± v·ªõi query
    
    Args:
        query (str): C√¢u h·ªèi c·∫ßn t√¨m
        collection_name (str): T√™n collection
        top_k (int): S·ªë l∆∞·ª£ng k·∫øt qu·∫£
        
    Returns:
        List[Dict]: Danh s√°ch k·∫øt qu·∫£
    """
    print(f"\nüîç ƒêang t√¨m ki·∫øm: '{query}'")
    
    # Kh·ªüi t·∫°o embeddings
    model = str(os.getenv("OPENAI_API_MODEL_NAME_EMBED"))
    base_url = os.getenv("OPENAI_BASE_URL_EMBED")
    api_key = str(os.getenv("OPENAI_API_KEY_EMBED"))
    
    embeddings = OpenAIEmbeddings(
        model=model,
        base_url=base_url,
        api_key=SecretStr(api_key),
        tiktoken_enabled=False,
    )
    
    # T·∫°o embedding cho query
    query_vector = embeddings.embed_query(query)
    
    # T√¨m ki·∫øm
    search_results = qdrant_client.search(
        collection_name=collection_name,
        query_vector=query_vector,
        limit=top_k,
        with_payload=True,
        with_vectors=False
    )
    
    # Format k·∫øt qu·∫£
    results = []
    for result in search_results:
        payload = result.payload or {}
        results.append({
            'seq_id': payload.get('seq_id', 'N/A'),
            'page_index': payload.get('page_index', 'N/A'),
            'seq_index': payload.get('seq_index', 'N/A'),
            'content': payload.get('content', ''),
            'word_count': payload.get('word_count', 'N/A'),
            'similarity_score': float(result.score)
        })
    
    return results


def display_retrieval_results(results: List[Dict]):
    """
    Hi·ªÉn th·ªã k·∫øt qu·∫£ retrieval
    
    Args:
        results (List[Dict]): K·∫øt qu·∫£ t·ª´ retrieve_similar_sequences
    """
    print(f"\nüìã K·∫æT QU·∫¢ RETRIEVAL (Top-{len(results)}):")
    print("=" * 80)
    
    for i, result in enumerate(results, 1):
        print(f"\n{i}. [{result['seq_id']}]")
        print(f"   üìÑ Page: {result['page_index']} | Sequence: {result['seq_index']}")
        print(f"   üìä Words: {result['word_count']} | Score: {result['similarity_score']:.4f}")
        print(f"   üìù Content:")
        
        # Hi·ªÉn th·ªã content v·ªõi wrap
        content = result['content']
        if len(content) > 200:
            print(f"      {content[:200]}...")
        else:
            print(f"      {content}")
    
    print("\n" + "=" * 80)


def display_statistics(sequences: List[Dict]):
    """
    Hi·ªÉn th·ªã th·ªëng k√™ v·ªÅ sequences
    
    Args:
        sequences (List[Dict]): List sequences
    """
    if not sequences:
        return
    
    total_sequences = len(sequences)
    total_words = sum(s['word_count'] for s in sequences)
    total_chars = sum(s['char_count'] for s in sequences)
    
    # Th·ªëng k√™ theo page
    pages = {}
    for seq in sequences:
        page = seq['page_index']
        if page not in pages:
            pages[page] = {'count': 0, 'words': 0}
        pages[page]['count'] += 1
        pages[page]['words'] += seq['word_count']
    
    print(f"\n{'='*80}")
    print("üìä TH·ªêNG K√ä SEQUENCES:")
    print("="*80)
    print(f"üìù T·ªïng sequences: {total_sequences:,}")
    print(f"üìÑ T·ªïng pages: {len(pages):,}")
    print(f"üí¨ T·ªïng t·ª´: {total_words:,}")
    print(f"üìä Trung b√¨nh: {total_words/total_sequences:.1f} t·ª´/sequence")
    print(f"üìè T·ªïng k√Ω t·ª±: {total_chars:,}")
    
    # Top pages c√≥ nhi·ªÅu sequences nh·∫•t
    top_pages = sorted(pages.items(), key=lambda x: x[1]['count'], reverse=True)[:5]
    print(f"\nüîù Top 5 pages c√≥ nhi·ªÅu sequences nh·∫•t:")
    for page_num, stats in top_pages:
        print(f"   Page {page_num}: {stats['count']} sequences, {stats['words']:,} words")
    
    print("="*80)


# ============================================================================
# MAIN PIPELINE
# ============================================================================

def run_sequences_pipeline(
    input_dir: str,
    output_json: str,
    collection_name: str = "esg_sequences",
    min_words: int = 10,
    batch_size: int = 50,
    skip_json: bool = False,
    skip_vectorstore: bool = False
):
    """
    Ch·∫°y pipeline ho√†n ch·ªânh: ƒë·ªçc sequences -> l∆∞u JSON -> l∆∞u vector store
    
    Args:
        input_dir (str): Th∆∞ m·ª•c ch·ª©a file txt
        output_json (str): File JSON output
        collection_name (str): T√™n collection Qdrant
        min_words (int): S·ªë t·ª´ t·ªëi thi·ªÉu cho sequence
        batch_size (int): Batch size khi upload
        skip_json (bool): B·ªè qua l∆∞u JSON
        skip_vectorstore (bool): B·ªè qua l∆∞u vector store
    """
    print("="*80)
    print("üöÄ SEQUENCES TO VECTOR STORE PIPELINE")
    print("="*80)
    print(f"üìÇ Input dir: {input_dir}")
    print(f"üìÑ Output JSON: {output_json}")
    print(f"üóÑÔ∏è Collection: {collection_name}")
    print(f"üî¢ Min words: {min_words}")
    print("="*80)
    
    # Step 1: ƒê·ªçc v√† t√°ch sequences
    print(f"\n{'='*80}")
    print("B∆Ø·ªöC 1: ƒê·ªåC V√Ä T√ÅCH SEQUENCES")
    print("="*80)
    
    sequences = read_sequences_from_directory(input_dir, min_words=min_words)
    
    if not sequences:
        print("‚ùå Kh√¥ng t√¨m th·∫•y sequences n√†o!")
        return False
    
    # Hi·ªÉn th·ªã th·ªëng k√™
    display_statistics(sequences)
    
    # Step 2: L∆∞u JSON
    if not skip_json:
        print(f"\n{'='*80}")
        print("B∆Ø·ªöC 2: L∆ØU SEQUENCES V√ÄO JSON")
        print("="*80)
        save_sequences_to_json(sequences, output_json)
    else:
        print(f"\n‚è≠Ô∏è B·ªè qua B∆Ø·ªöC 2: L∆∞u JSON (skip_json=True)")
    
    # Step 3: L∆∞u v√†o Vector Store
    if not skip_vectorstore:
        print(f"\n{'='*80}")
        print("B∆Ø·ªöC 3: L∆ØU V√ÄO QDRANT VECTOR STORE")
        print("="*80)
        
        try:
            vectorstore = store_sequences_in_qdrant(
                sequences, 
                collection_name=collection_name,
                batch_size=batch_size
            )
            
            # Step 4: Test retrieval
            print(f"\n{'='*80}")
            print("B∆Ø·ªöC 4: TEST RETRIEVAL")
            print("="*80)
            
            test_queries = [
                "v·ªën ƒëi·ªÅu l·ªá c·ªßa c√¥ng ty",
                "b√°o c√°o t√†i ch√≠nh nƒÉm 2024",
                "ho·∫°t ƒë·ªông kinh doanh ch√≠nh",
                "qu·∫£n tr·ªã r·ªßi ro v√† tu√¢n th·ªß",
                "ph√°t tri·ªÉn b·ªÅn v·ªØng ESG"
            ]
            
            for query in test_queries:
                results = retrieve_similar_sequences(
                    query, 
                    collection_name=collection_name,
                    top_k=3
                )
                display_retrieval_results(results)
            
        except Exception as e:
            print(f"‚ùå L·ªói khi l∆∞u v√†o vector store: {e}")
            import traceback
            traceback.print_exc()
            return False
    else:
        print(f"\n‚è≠Ô∏è B·ªè qua B∆Ø·ªöC 3-4: Vector store (skip_vectorstore=True)")
    
    # Final summary
    print(f"\n{'='*80}")
    print("‚úÖ PIPELINE HO√ÄN TH√ÄNH!")
    print("="*80)
    print(f"üìù T·ªïng sequences: {len(sequences):,}")
    if not skip_json:
        print(f"üìÑ JSON saved: {output_json}")
    if not skip_vectorstore:
        print(f"üóÑÔ∏è Vector store: {collection_name}")
    print("="*80)
    
    return True


# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":
    # Load environment variables
    from dotenv import load_dotenv
    load_dotenv()
    
    # C·∫•u h√¨nh
    INPUT_DIR = project_root / "src" / "data" / "raw"
    OUTPUT_JSON = project_root / "src" / "data" / "push" / "sequences_data.json"
    COLLECTION_NAME = "esg_sequences"
    MIN_WORDS = 10  # S·ªë t·ª´ t·ªëi thi·ªÉu cho m·ªôt sequence
    BATCH_SIZE = 50  # S·ªë sequences m·ªói batch khi upload
    
    # Flags
    SKIP_JSON = False  # True n·∫øu kh√¥ng c·∫ßn l∆∞u JSON
    SKIP_VECTORSTORE = False  # True n·∫øu ch·ªâ mu·ªën t√°ch sequences
    
    # Ch·∫°y pipeline
    success = run_sequences_pipeline(
        input_dir=str(INPUT_DIR),
        output_json=str(OUTPUT_JSON),
        collection_name=COLLECTION_NAME,
        min_words=MIN_WORDS,
        batch_size=BATCH_SIZE,
        skip_json=SKIP_JSON,
        skip_vectorstore=SKIP_VECTORSTORE
    )
    
    if success:
        print("\nüéâ T·∫•t c·∫£ c√°c b∆∞·ªõc ƒë√£ ho√†n th√†nh th√†nh c√¥ng!")
    else:
        print("\n‚ùå Pipeline g·∫∑p l·ªói. Vui l√≤ng ki·ªÉm tra log.")
