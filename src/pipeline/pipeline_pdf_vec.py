import os
import sys
import json
import uuid
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

# PDF processing
from pdfminer.high_level import extract_text as pdfminer_extract_text
from pdfminer.layout import LAParams
from io import StringIO
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Add project root to path
script_dir = Path(__file__).resolve().parent
project_root = script_dir.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Import c√°c components
from src.models.halong_embedd import HalongEmbedding
from src.models.embedd import QwenEmbedding
from src.models.dangvantuan_embedd import DangVanTuanEmbedding

from src.helpers.init_qdrant import qdrant_client
from qdrant_client.models import PointStruct, VectorParams, Distance


@dataclass
class PDFPage:
    """C·∫•u tr√∫c d·ªØ li·ªáu cho m·ªôt trang PDF"""
    page_index: int
    content: str
    word_count: int
    file_name: str
    sequence_id: str


@dataclass
class PDFDocument:
    """C·∫•u tr√∫c d·ªØ li·ªáu cho to√†n b·ªô document PDF"""
    file_path: str
    file_name: str
    total_pages: int
    pages: List[PDFPage]
    total_words: int

class PDFTextExtractor:
    """Tr√≠ch xu·∫•t text t·ª´ PDF files"""

    def __init__(self):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=2000,  # Gi·ªõi h·∫°n ƒë·ªô d√†i text cho embedding
            chunk_overlap=200,
            separators=["\n\n", "\n", ". ", " ", ""]
        )

    def extract_text_from_pdf(self, pdf_path: str) -> PDFDocument:
        """
        Tr√≠ch xu·∫•t text t·ª´ PDF file s·ª≠ d·ª•ng pdfminer

        Args:
            pdf_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file PDF

        Returns:
            PDFDocument: Document v·ªõi th√¥ng tin chi ti·∫øt
        """
        pdf_path_obj = Path(pdf_path)
        if not pdf_path_obj.exists():
            raise FileNotFoundError(f"PDF file not found: {pdf_path_obj}")

        print(f"üìñ ƒêang ƒë·ªçc PDF: {pdf_path_obj.name}")

        # C·∫•u h√¨nh pdfminer
        laparams = LAParams(
            line_margin=0.5,
            word_margin=0.1,
            char_margin=2.0,
            boxes_flow=0.5
        )

        # Tr√≠ch xu·∫•t to√†n b·ªô text t·ª´ PDF
        try:
            full_text = pdfminer_extract_text(pdf_path, laparams=laparams)
        except Exception as e:
            raise RuntimeError(f"Failed to extract text from PDF: {e}")

        # L√†m s·∫°ch text
        full_text = self._clean_text(full_text)

        if not full_text.strip():
            raise ValueError("No text content found in PDF")

        # Chia text th√†nh c√°c chunks
        text_chunks = self.text_splitter.split_text(full_text)

        # T·∫°o pages t·ª´ chunks (pdfminer kh√¥ng ph√¢n bi·ªát pages r√µ r√†ng)
        pages = []
        for chunk_idx, chunk in enumerate(text_chunks):
            word_count = len(chunk.split())

            # T·∫°o sequence ID (kh√¥ng c√≥ th√¥ng tin page c·ª• th·ªÉ t·ª´ pdfminer)
            sequence_id = f"{pdf_path_obj.stem}_seq_{chunk_idx + 1}"

            pdf_page = PDFPage(
                page_index=chunk_idx + 1,  # S·ª≠ d·ª•ng chunk index l√†m page index
                content=chunk,
                word_count=word_count,
                file_name=pdf_path_obj.name,
                sequence_id=sequence_id
            )
            pages.append(pdf_page)

        # T√≠nh t·ªïng s·ªë t·ª´
        total_words = sum(page.word_count for page in pages)

        # ∆Ø·ªõc t√≠nh s·ªë trang (d·ª±a tr√™n s·ªë chunks)
        estimated_pages = max(1, len(text_chunks) // 3)  # ∆Ø·ªõc t√≠nh kho·∫£ng 3 chunks per page

        pdf_doc = PDFDocument(
            file_path=str(pdf_path_obj),
            file_name=pdf_path_obj.name,
            total_pages=estimated_pages,
            pages=pages,
            total_words=total_words
        )

        print(f"‚úÖ ƒê√£ tr√≠ch xu·∫•t {len(pages)} sequences, {total_words} t·ª´ t·ª´ PDF")
        return pdf_doc

    def _clean_text(self, text: str) -> str:
        """L√†m s·∫°ch text ƒë√£ tr√≠ch xu·∫•t"""
        if not text:
            return ""

        # Lo·∫°i b·ªè c√°c d√≤ng tr·ªëng li√™n ti·∫øp
        lines = text.split('\n')
        cleaned_lines = []

        for line in lines:
            line = line.strip()
            if line:  # Ch·ªâ gi·ªØ l·∫°i d√≤ng c√≥ n·ªôi dung
                cleaned_lines.append(line)

        # Gh√©p l·∫°i th√†nh text
        cleaned_text = '\n'.join(cleaned_lines)

        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
        cleaned_text = ' '.join(cleaned_text.split())

        return cleaned_text


class PDFVectorStorePipeline:
    """Pipeline ho√†n ch·ªânh t·ª´ PDF ƒë·∫øn Vector Store"""

    def __init__(self, embedding_model='qwen', collection_name: str = "esg_sequences"):
        """
        Kh·ªüi t·∫°o pipeline

        Args:
            embedding_model (str): Model embedding ('halong', 'qwen', 'dangvantuan')
            collection_name (str): T√™n collection trong Qdrant
        """
        self.collection_name = collection_name
        self.extractor = PDFTextExtractor()
        self.embedding = self._load_embedding_model(embedding_model)

        print(f"üîß Pipeline initialized with {embedding_model} embedding and {collection_name} collection")

    def _load_embedding_model(self, model_name: str):
        """Load embedding model"""
        if model_name.lower() == 'qwen':
            return QwenEmbedding()
        elif model_name.lower() == 'dangvantuan':
            return DangVanTuanEmbedding()
        else:
            raise ValueError(f"Unknown embedding model: {model_name}")

    def process_single_pdf(self, pdf_path: str, use_text_correction: bool = False) -> PDFDocument:
        """
        X·ª≠ l√Ω m·ªôt file PDF

        Args:
            pdf_path (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn PDF
            use_text_correction (bool): C√≥ s·ª≠ d·ª•ng LLM text correction kh√¥ng

        Returns:
            PDFDocument: Document ƒë√£ x·ª≠ l√Ω
        """
        # Tr√≠ch xu·∫•t text t·ª´ PDF
        pdf_doc = self.extractor.extract_text_from_pdf(pdf_path)

        # √Åp d·ª•ng text correction n·∫øu ƒë∆∞·ª£c y√™u c·∫ßu
        if use_text_correction:
            pdf_doc = self._apply_text_correction(pdf_doc)

        return pdf_doc

    def _apply_text_correction(self, pdf_doc: PDFDocument) -> PDFDocument:
        """√Åp d·ª•ng LLM text correction cho document"""
        print("üîß Applying text correction...")

        try:
            # Import text correction module
            from src.helpers.llm_text_correction import correct_vietnamese

            corrected_pages = []
            for page in pdf_doc.pages:
                # √Åp d·ª•ng correction cho t·ª´ng page
                corrected_content = correct_vietnamese(
                    text=page.content,
                    repeat_reminder=1,
                    model=None,  # S·∫Ω ƒë∆∞·ª£c load trong function
                    tokenizer=None,
                    use_enhanced_prompt=True
                )

                # T·∫°o page m·ªõi v·ªõi n·ªôi dung ƒë√£ s·ª≠a
                corrected_page = PDFPage(
                    page_index=page.page_index,
                    content=corrected_content,
                    word_count=len(corrected_content.split()),
                    file_name=page.file_name,
                    sequence_id=page.sequence_id
                )
                corrected_pages.append(corrected_page)

            # C·∫≠p nh·∫≠t document
            pdf_doc.pages = corrected_pages
            pdf_doc.total_words = sum(page.word_count for page in corrected_pages)

            print("‚úÖ Text correction applied")

        except Exception as e:
            print(f"‚ö†Ô∏è Text correction failed: {e}")
            print("Continuing without text correction...")

        return pdf_doc

    def upload_to_vectorstore(self, pdf_doc: PDFDocument, batch_size: int = 50) -> Dict[str, Any]:
        """
        Upload document l√™n Qdrant vector store - x·ª≠ l√Ω t·ª´ng page ri√™ng bi·ªát

        Args:
            pdf_doc (PDFDocument): Document c·∫ßn upload
            batch_size (int): K√≠ch th∆∞·ªõc batch (deprecated, gi·ªØ ƒë·ªÉ t∆∞∆°ng th√≠ch)

        Returns:
            Dict[str, Any]: Th√¥ng tin upload
        """
        print(f"üì§ Uploading {len(pdf_doc.pages)} sequences to {self.collection_name}...")

        # Chu·∫©n b·ªã collection
        self._prepare_collection()

        uploaded_count = 0

        # X·ª≠ l√Ω t·ª´ng page ri√™ng bi·ªát
        for i, page in enumerate(pdf_doc.pages, 1):
            try:
                # T·∫°o embedding cho page n√†y
                embedding = self.embedding.get_embedding(page.content)

                if embedding is not None:
                    # T·∫°o payload
                    payload = {
                        'sequence_id': page.sequence_id,
                        'content': page.content,
                        'file_name': page.file_name,
                        'page_index': page.page_index,
                        'word_count': page.word_count,
                        'total_sequences_in_file': len(pdf_doc.pages)
                    }

                    # T·∫°o point v·ªõi ID l√† integer theo th·ª© t·ª±
                    point_id = uploaded_count + 1  # ID t·ª´ 1, 2, 3...
                    point = PointStruct(
                        id=point_id,
                        vector=embedding.tolist(),
                        payload=payload
                    )

                    # Upload ngay l·∫≠p t·ª©c
                    qdrant_client.upsert(
                        collection_name=self.collection_name,
                        points=[point]  # Upload t·ª´ng point m·ªôt
                    )

                    uploaded_count += 1

                    if i % 10 == 0 or i == len(pdf_doc.pages):
                        print(f"  üì¶ Uploaded {i}/{len(pdf_doc.pages)} sequences")

            except Exception as e:
                print(f"  ‚ö†Ô∏è Failed to upload sequence {page.sequence_id}: {e}")
                continue

        result = {
            'collection_name': self.collection_name,
            'uploaded_sequences': uploaded_count,
            'total_pages': pdf_doc.total_pages,
            'total_words': pdf_doc.total_words,
            'file_name': pdf_doc.file_name
        }

        print(f"‚úÖ Successfully uploaded {uploaded_count} sequences to {self.collection_name}")
        return result

    def _prepare_collection(self):
        """Chu·∫©n b·ªã collection trong Qdrant"""
        try:
            # L·∫•y sample embedding ƒë·ªÉ x√°c ƒë·ªãnh vector size
            sample_text = "test embedding"
            sample_embedding = self.embedding.get_embedding(sample_text)

            if sample_embedding is None:
                raise ValueError("Cannot create embedding for sample text")

            vector_size = len(sample_embedding)
            print(f"üìè Embedding vector size: {vector_size}")

            # Ki·ªÉm tra collection c√≥ t·ªìn t·∫°i kh√¥ng
            collections = qdrant_client.get_collections()
            collection_names = [c.name for c in collections.collections]

            if self.collection_name in collection_names:
                print(f"üìÅ Collection '{self.collection_name}' already exists")
                # Th·ª≠ upload sample ƒë·ªÉ ki·ªÉm tra dimension
                try:
                    test_point = PointStruct(
                        id=999999,  # ID test
                        vector=sample_embedding.tolist(),
                        payload={'test': True}
                    )
                    qdrant_client.upsert(
                        collection_name=self.collection_name,
                        points=[test_point]
                    )
                    # N·∫øu th√†nh c√¥ng, x√≥a test point
                    qdrant_client.delete(
                        collection_name=self.collection_name,
                        points_selector=[999999]
                    )
                    print("‚úÖ Collection dimension matches")
                except Exception as e:
                    if "Vector dimension error" in str(e):
                        print(f"‚ö†Ô∏è Collection dimension mismatch, deleting and recreating...")
                        # X√≥a collection c≈©
                        qdrant_client.delete_collection(self.collection_name)
                        print(f"‚úÖ Deleted old collection '{self.collection_name}'")

                        # T·∫°o collection m·ªõi
                        qdrant_client.create_collection(
                            collection_name=self.collection_name,
                            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
                        )
                        print(f"üÜï Created new collection '{self.collection_name}' with vector size {vector_size}")
                    else:
                        raise e
            else:
                # T·∫°o collection m·ªõi
                qdrant_client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE)
                )
                print(f"üÜï Created new collection '{self.collection_name}' with vector size {vector_size}")

        except Exception as e:
            print(f"‚ùå Error preparing collection: {e}")
            raise

    def process_directory(self, pdf_dir: str, use_text_correction: bool = False) -> List[Dict[str, Any]]:
        """
        X·ª≠ l√Ω t·∫•t c·∫£ PDF files trong th∆∞ m·ª•c

        Args:
            pdf_dir (str): Th∆∞ m·ª•c ch·ª©a PDF files
            use_text_correction (bool): C√≥ s·ª≠ d·ª•ng text correction kh√¥ng

        Returns:
            List[Dict[str, Any]]: K·∫øt qu·∫£ x·ª≠ l√Ω cho t·ª´ng file
        """
        pdf_dir_obj = Path(pdf_dir)
        if not pdf_dir_obj.exists():
            raise FileNotFoundError(f"Directory not found: {pdf_dir_obj}")

        # T√¨m t·∫•t c·∫£ file PDF
        pdf_files = list(pdf_dir_obj.glob("*.pdf"))
        print(f"üìÇ Found {len(pdf_files)} PDF files in {pdf_dir_obj}")

        results = []
        for pdf_file in pdf_files:
            try:
                print(f"\nüîÑ Processing: {pdf_file.name}")

                # X·ª≠ l√Ω PDF
                pdf_doc = self.process_single_pdf(str(pdf_file), use_text_correction)

                # Upload l√™n vector store
                upload_result = self.upload_to_vectorstore(pdf_doc)

                # K·∫øt h·ª£p k·∫øt qu·∫£
                result = {
                    'file_name': pdf_file.name,
                    'status': 'success',
                    'sequences_extracted': len(pdf_doc.pages),
                    'total_words': pdf_doc.total_words,
                    'upload_info': upload_result
                }
                results.append(result)

                print(f"‚úÖ Completed: {pdf_file.name}")

            except Exception as e:
                print(f"‚ùå Failed to process {pdf_file.name}: {e}")
                results.append({
                    'file_name': pdf_file.name,
                    'status': 'error',
                    'error': str(e)
                })

        return results

    def save_results(self, results: List[Dict[str, Any]], output_file: str):
        """L∆∞u k·∫øt qu·∫£ x·ª≠ l√Ω ra file JSON"""
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"üíæ Results saved to {output_file}")


def main():
    """Main pipeline function"""
    print("üöÄ PDF TO VECTOR STORE PIPELINE")
    print("=" * 50)

    # C·∫•u h√¨nh
    script_dir = Path(__file__).resolve().parent
    project_root = script_dir.parent.parent

    # Th∆∞ m·ª•c input (PDF files)
    pdf_input_dir = project_root / "data" / "pdfs"  # Th∆∞ m·ª•c ch·ª©a PDF files

    # File output
    results_file = project_root / "data" / "pipeline_results.json"

    # Kh·ªüi t·∫°o pipeline
    pipeline = PDFVectorStorePipeline(
        embedding_model='qwen',  # C√≥ th·ªÉ ƒë·ªïi th√†nh 'qwen' ho·∫∑c 'dangvantuan'
        collection_name='esg_sequences'
    )

    # Ki·ªÉm tra th∆∞ m·ª•c input
    if not pdf_input_dir.exists():
        print(f"‚ùå PDF directory not found: {pdf_input_dir}")
        print("Please create the directory and add PDF files")
        return

    # X·ª≠ l√Ω t·∫•t c·∫£ PDF trong th∆∞ m·ª•c
    print(f"üìÇ Processing PDFs from: {pdf_input_dir}")
    results = pipeline.process_directory(
        pdf_dir=str(pdf_input_dir),
        use_text_correction=False  # C√≥ th·ªÉ b·∫≠t True n·∫øu mu·ªën d√πng LLM correction
    )

    # L∆∞u k·∫øt qu·∫£
    pipeline.save_results(results, str(results_file))

    # Th·ªëng k√™
    successful = [r for r in results if r['status'] == 'success']
    failed = [r for r in results if r['status'] == 'error']

    print(f"\nüìä SUMMARY:")
    print(f"   ‚úÖ Successful: {len(successful)}")
    print(f"   ‚ùå Failed: {len(failed)}")

    if successful:
        total_sequences = sum(r['sequences_extracted'] for r in successful)
        total_words = sum(r['total_words'] for r in successful)
        print(f"   üìÑ Total sequences: {total_sequences}")
        print(f"   üìù Total words: {total_words}")

    if failed:
        print(f"   Failed files: {[r['file_name'] for r in failed]}")

    print(f"\nüéâ Pipeline completed! Results saved to {results_file}")


if __name__ == "__main__":
    main()